Prompt engineering
This guide shares strategies and tactics for getting better results from large language models (sometimes referred to as GPT models) like GPT-4o. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.

You can also explore example prompts which showcase what our models are capable of:

Prompt examples
Explore prompt examples to learn what GPT models can do

Six strategies for getting better results
Write clear instructions
These models can‚Äôt read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you‚Äôd like to see. The less the model has to guess at what you want, the more likely you‚Äôll get it.

Tactics:

Include details in your query to get more relevant answers
Ask the model to adopt a persona
Use delimiters to clearly indicate distinct parts of the input
Specify the steps required to complete a task
Provide examples
Specify the desired length of the output
Provide reference text
Language models can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to these models can help in answering with fewer fabrications.

Tactics:

Instruct the model to answer using a reference text
Instruct the model to answer with citations from a reference text
Split complex tasks into simpler subtasks
Just as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to a language model. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.

Tactics:

Use intent classification to identify the most relevant instructions for a user query
For dialogue applications that require very long conversations, summarize or filter previous dialogue
Summarize long documents piecewise and construct a full summary recursively
Give the model time to "think"
If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, models make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a "chain of thought" before an answer can help the model reason its way toward correct answers more reliably.

Tactics:

Instruct the model to work out its own solution before rushing to a conclusion
Use inner monologue or a sequence of queries to hide the model's reasoning process
Ask the model if it missed anything on previous passes
Use external tools
Compensate for the weaknesses of the model by feeding it the outputs of other tools. For example, a text retrieval system (sometimes called RAG or retrieval augmented generation) can tell the model about relevant documents. A code execution engine like OpenAI's Code Interpreter can help the model do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a language model, offload it to get the best of both.

Tactics:

Use embeddings-based search to implement efficient knowledge retrieval
Use code execution to perform more accurate calculations or call external APIs
Give the model access to specific functions
Test changes systematically
Improving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an "eval").

Tactic:

Evaluate model outputs with reference to gold-standard answers
Tactics
Each of the strategies listed above can be instantiated with specific tactics. These tactics are meant to provide ideas for things to try. They are by no means fully comprehensive, and you should feel free to try creative ideas not represented here.

Strategy: Write clear instructions
Tactic: Include details in your query to get more relevant answers
In order to get a highly relevant response, make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.

Worse	Better
How do I add numbers in Excel?	How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called "Total".
Who‚Äôs president?	Who was the president of Mexico in 2021, and how frequently are elections held?
Write code to calculate the Fibonacci sequence.	Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.
Summarize the meeting notes.	Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.
Tactic: Ask the model to adopt a persona
The system message can be used to specify the persona used by the model in its replies.

SYSTEM
When I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.
USER
Write a thank you note to my steel bolt vendor for getting the delivery in on time and in short notice. This made it possible for us to deliver an important order.
Tactic: Use delimiters to clearly indicate distinct parts of the input
Delimiters like triple quotation marks, XML tags, section titles, etc. can help demarcate sections of text to be treated differently.

USER
Summarize the text delimited by triple quotes with a haiku.

"""insert text here"""
SYSTEM
You will be provided with a pair of articles (delimited with XML tags) about the same topic. First summarize the arguments of each article. Then indicate which of them makes a better argument and explain why.
USER
<article> insert first article here </article>

<article> insert second article here </article>
SYSTEM
You will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives.
USER
Abstract: insert abstract here

Title: insert title here
For straightforward tasks such as these, using delimiters might not make a difference in the output quality. However, the more complex a task is the more important it is to disambiguate task details. Don‚Äôt make the model work to understand exactly what you are asking of them.

Tactic: Specify the steps required to complete a task
Some tasks are best specified as a sequence of steps. Writing the steps out explicitly can make it easier for the model to follow them.

SYSTEM
Use the following step-by-step instructions to respond to user inputs.

Step 1 - The user will provide you with text in triple quotes. Summarize this text in one sentence with a prefix that says "Summary: ".

Step 2 - Translate the summary from Step 1 into Spanish, with a prefix that says "Translation: ".
USER
"""insert text here"""
Tactic: Provide examples
Providing general instructions that apply to all examples is generally more efficient than demonstrating all permutations of a task by example, but in some cases providing examples may be easier. For example, if you intend for the model to copy a particular style of responding to user queries which is difficult to describe explicitly. This is known as "few-shot" prompting.

SYSTEM
Answer in a consistent style.
USER
Teach me about patience.
ASSISTANT
The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.
USER
Teach me about the ocean.
Tactic: Specify the desired length of the output
You can ask the model to produce outputs that are of a given target length. The targeted output length can be specified in terms of the count of words, sentences, paragraphs, bullet points, etc. Note however that instructing the model to generate a specific number of words does not work with high precision. The model can more reliably generate outputs with a specific number of paragraphs or bullet points.

USER
Summarize the text delimited by triple quotes in about 50 words.

"""insert text here"""
USER
Summarize the text delimited by triple quotes in 2 paragraphs.

"""insert text here"""
USER
Summarize the text delimited by triple quotes in 3 bullet points.

"""insert text here"""
Strategy: Provide reference text
Tactic: Instruct the model to answer using a reference text
If we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer.

SYSTEM
Use the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write "I could not find an answer."
USER
<insert articles, each delimited by triple quotes>

Question: <insert question here>
Given that all models have limited context windows, we need some way to dynamically lookup information that is relevant to the question being asked. Embeddings can be used to implement efficient knowledge retrieval. See the tactic "Use embeddings-based search to implement efficient knowledge retrieval" for more details on how to implement this.

Tactic: Instruct the model to answer with citations from a reference text
If the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.

SYSTEM
You will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: "Insufficient information." If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({"citation": ‚Ä¶}).
USER
"""<insert document here>"""

Question: <insert question here>
Strategy: Split complex tasks into simpler subtasks
Tactic: Use intent classification to identify the most relevant instructions for a user query
For tasks in which lots of independent sets of instructions are needed to handle different cases, it can be beneficial to first classify the type of query and to use that classification to determine which instructions are needed. This can be achieved by defining fixed categories and hardcoding instructions that are relevant for handling tasks in a given category. This process can also be applied recursively to decompose a task into a sequence of stages. The advantage of this approach is that each query will contain only those instructions that are required to perform the next stage of a task which can result in lower error rates compared to using a single query to perform the whole task. This can also result in lower costs since larger prompts cost more to run (see pricing information).

Suppose for example that for a customer service application, queries could be usefully classified as follows:

SYSTEM
You will be provided with customer service queries. Classify each query into a primary category and a secondary category. Provide your output in json format with the keys: primary and secondary.

Primary categories: Billing, Technical Support, Account Management, or General Inquiry.

Billing secondary categories:
- Unsubscribe or upgrade
- Add a payment method
- Explanation for charge
- Dispute a charge

Technical Support secondary categories:
- Troubleshooting
- Device compatibility
- Software updates

Account Management secondary categories:
- Password reset
- Update personal information
- Close account
- Account security

General Inquiry secondary categories:
- Product information
- Pricing
- Feedback
- Speak to a human
USER
I need to get my internet working again.
Based on the classification of the customer query, a set of more specific instructions can be provided to a model for it to handle next steps. For example, suppose the customer requires help with "troubleshooting".

SYSTEM
You will be provided with customer service inquiries that require troubleshooting in a technical support context. Help the user by:

- Ask them to check that all cables to/from the router are connected. Note that it is common for cables to come loose over time.
- If all cables are connected and the issue persists, ask them which router model they are using
- Now you will advise them how to restart their device:
-- If the model number is MTD-327J, advise them to push the red button and hold it for 5 seconds, then wait 5 minutes before testing the connection.
-- If the model number is MTD-327S, advise them to unplug and replug it, then wait 5 minutes before testing the connection.
- If the customer's issue persists after restarting the device and waiting 5 minutes, connect them to IT support by outputting {"IT support requested"}.
- If the user starts asking questions that are unrelated to this topic then confirm if they would like to end the current chat about troubleshooting and classify their request according to the following scheme:

<insert primary/secondary classification scheme from above here>
USER
I need to get my internet working again.
Notice that the model has been instructed to emit special strings to indicate when the state of the conversation changes. This enables us to turn our system into a state machine where the state determines which instructions are injected. By keeping track of state, what instructions are relevant at that state, and also optionally what state transitions are allowed from that state, we can put guardrails around the user experience that would be hard to achieve with a less structured approach.

Tactic: For dialogue applications that require very long conversations, summarize or filter previous dialogue
Since models have a fixed context length, dialogue between a user and an assistant in which the entire conversation is included in the context window cannot continue indefinitely.

There are various workarounds to this problem, one of which is to summarize previous turns in the conversation. Once the size of the input reaches a predetermined threshold length, this could trigger a query that summarizes part of the conversation and the summary of the prior conversation could be included as part of the system message. Alternatively, prior conversation could be summarized asynchronously in the background throughout the entire conversation.

An alternative solution is to dynamically select previous parts of the conversation that are most relevant to the current query. See the tactic "Use embeddings-based search to implement efficient knowledge retrieval".

Tactic: Summarize long documents piecewise and construct a full summary recursively
Since models have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query.

To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it‚Äôs necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. The effectiveness of this procedure for summarizing books has been studied in previous research by OpenAI using variants of GPT-3.

Strategy: Give models time to "think"
Tactic: Instruct the model to work out its own solution before rushing to a conclusion
Sometimes we get better results when we explicitly instruct the model to reason from first principles before coming to a conclusion. Suppose for example we want a model to evaluate a student‚Äôs solution to a math problem. The most obvious way to approach this is to simply ask the model if the student's solution is correct or not.

SYSTEM
Determine if the student's solution is correct or not.
USER
Problem Statement: I'm building a solar power installation and I need help working out the financials.
- Land costs $100 / square foot
- I can buy solar panels for $250 / square foot
- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot
What is the total cost for the first year of operations as a function of the number of square feet.

Student's Solution: Let x be the size of the installation in square feet.
1. Land cost: 100x
2. Solar panel cost: 250x
3. Maintenance cost: 100,000 + 100x
Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000
ASSISTANT
The student's solution is correct.
But the student's solution is actually not correct! We can get the model to successfully notice this by prompting the model to generate its own solution first.

SYSTEM
First work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.
USER
Problem Statement: I'm building a solar power installation and I need help working out the financials.
- Land costs $100 / square foot
- I can buy solar panels for $250 / square foot
- I negotiated a contract for maintenance that will cost me a flat $100k per year, and an additional $10 / square foot
What is the total cost for the first year of operations as a function of the number of square feet.

Student's Solution: Let x be the size of the installation in square feet.
1. Land cost: 100x
2. Solar panel cost: 250x
3. Maintenance cost: 100,000 + 100x
Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000
ASSISTANT
Let x be the size of the installation in square feet.
1. Land cost: 100x
2. Solar panel cost: 250x
3. Maintenance cost: 100,000 + 10x
Total cost: 100x + 250x + 100,000 + 10x = 360x + 100,000

The student's solution is incorrect. They made an error in the maintenance cost calculation, using 100x instead of 10x. The correct total cost for the first year of operations is 360x + 100,000.
Tactic: Use inner monologue or a sequence of queries to hide the model's reasoning process
The previous tactic demonstrates that it is sometimes important for the model to reason in detail about a problem before answering a specific question. For some applications, the reasoning process that a model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications we may want to encourage students to work out their own answers, but a model‚Äôs reasoning process about the student‚Äôs solution could reveal the answer to the student.

Inner monologue is a tactic that can be used to mitigate this. The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes parsing them easy. Then before presenting the output to the user, the output is parsed and only part of the output is made visible.

SYSTEM
Follow these steps to answer the user queries.

Step 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (""").

Step 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (""").

Step 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (""").

Step 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing "Step 4 - ..." write "Hint:".
USER
Problem Statement: <insert problem statement>

Student Solution: <insert student solution>
Alternatively, this can be achieved with a sequence of queries in which all except the last have their output hidden from the end user.

First, we can ask the model to solve the problem on its own. Since this initial query doesn't require the student‚Äôs solution, it can be omitted. This provides the additional advantage that there is no chance that the model‚Äôs solution will be biased by the student‚Äôs attempted solution.

USER
<insert problem statement>
Next, we can have the model use all available information to assess the correctness of the student‚Äôs solution.

SYSTEM
Compare your solution to the student's solution and evaluate if the student's solution is correct or not.
USER
Problem statement: """<insert problem statement>"""

Your solution: """<insert model generated solution>"""

Student‚Äôs solution: """<insert student's solution>"""
Finally, we can let the model use its own analysis to construct a reply in the persona of a helpful tutor.

SYSTEM
You are a math tutor. If the student made an error, offer a hint to the student in a way that does not reveal the answer. If the student did not make an error, simply offer them an encouraging comment.
USER
Problem statement: """<insert problem statement>"""

Your solution: """<insert model generated solution>"""

Student‚Äôs solution: """<insert student's solution>"""

Analysis: """<insert model generated analysis from previous step>"""
Tactic: Ask the model if it missed anything on previous passes
Suppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.

SYSTEM
You will be provided with a document delimited by triple quotes. Your task is to select excerpts which pertain to the following question: "What significant paradigm shifts have occurred in the history of artificial intelligence."

Ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context. Provide output in JSON format as follows:

[{"excerpt": "..."},
...
{"excerpt": "..."}]
USER
"""<insert document here>"""
ASSISTANT
[{"excerpt": "the model writes an excerpt here"},
...
{"excerpt": "the model writes another excerpt here"}]
USER
Are there more relevant excerpts? Take care not to repeat excerpts. Also ensure that excerpts contain all relevant context needed to interpret them - in other words don't extract small snippets that are missing important context.
Strategy: Use external tools
Tactic: Use embeddings-based search to implement efficient knowledge retrieval
A model can leverage external sources of information if provided as part of its input. This can help the model to generate more informed and up-to-date responses. For example, if a user asks a question about a specific movie, it may be useful to add high quality information about the movie (e.g. actors, director, etc‚Ä¶) to the model‚Äôs input. Embeddings can be used to implement efficient knowledge retrieval, so that relevant information can be added to the model input dynamically at run-time.

A text embedding is a vector that can measure the relatedness between text strings. Similar or relevant strings will be closer together than unrelated strings. This fact, along with the existence of fast vector search algorithms means that embeddings can be used to implement efficient knowledge retrieval. In particular, a text corpus can be split up into chunks, and each chunk can be embedded and stored. Then a given query can be embedded and vector search can be performed to find the embedded chunks of text from the corpus that are most related to the query (i.e. closest together in the embedding space).

Example implementations can be found in the OpenAI Cookbook. See the tactic ‚ÄúInstruct the model to use retrieved knowledge to answer queries‚Äù for an example of how to use knowledge retrieval to minimize the likelihood that a model will make up incorrect facts.

Tactic: Use code execution to perform more accurate calculations or call external APIs
Language models cannot be relied upon to perform arithmetic or long calculations accurately on their own. In cases where this is needed, a model can be instructed to write and run code instead of making its own calculations. In particular, a model can be instructed to put code that is meant to be run into a designated format such as triple backtick. After an output is produced, the code can be extracted and run. Finally, if necessary, the output from the code execution engine (i.e. Python interpreter) can be provided as an input to the model for the next query.

SYSTEM
You can write and execute Python code by enclosing it in triple backticks, e.g. ```code goes here```. Use this to perform calculations.
USER
Find all real-valued roots of the following polynomial: 3*x**5 - 5*x**4 - 3*x**3 - 7*x - 10.
Another good use case for code execution is calling external APIs. If a model is instructed in the proper use of an API, it can write code that makes use of it. A model can be instructed in how to use an API by providing it with documentation and/or code samples showing how to use the API.

SYSTEM
You can write and execute Python code by enclosing it in triple backticks. Also note that you have access to the following module to help users send messages to their friends:

```python
import message
message.write(to="John", message="Hey, want to meetup after work?")```
WARNING: Executing code produced by a model is not inherently safe and precautions should be taken in any application that seeks to do this. In particular, a sandboxed code execution environment is needed to limit the harm that untrusted code could cause.

Tactic: Give the model access to specific functions
The Chat Completions API allows passing a list of function descriptions in requests. This enables models to generate function arguments according to the provided schemas. Generated function arguments are returned by the API in JSON format and can be used to execute function calls. Output provided by function calls can then be fed back into a model in the following request to close the loop. This is the recommended way of using OpenAI models to call external functions. To learn more see the function calling section in our introductory text generation guide and more function calling examples in the OpenAI Cookbook.

Strategy: Test changes systematically
Sometimes it can be hard to tell whether a change ‚Äî e.g., a new instruction or a new design ‚Äî makes your system better or worse. Looking at a few examples may hint at which is better, but with small sample sizes it can be hard to distinguish between a true improvement or random luck. Maybe the change helps performance on some inputs, but hurts performance on others.

Evaluation procedures (or "evals") are useful for optimizing system designs. Good evals are:

Representative of real-world usage (or at least diverse)
Contain many test cases for greater statistical power (see table below for guidelines)
Easy to automate or repeat
DIFFERENCE TO DETECT	SAMPLE SIZE NEEDED FOR 95% CONFIDENCE
30%	~10
10%	~100
3%	~1,000
1%	~10,000
Evaluation of outputs can be done by computers, humans, or a mix. Computers can automate evals with objective criteria (e.g., questions with single correct answers) as well as some subjective or fuzzy criteria, in which model outputs are evaluated by other model queries. OpenAI Evals is an open-source software framework that provides tools for creating automated evals.

Model-based evals can be useful when there exists a range of possible outputs that would be considered equally high in quality (e.g. for questions with long answers). The boundary between what can be realistically evaluated with a model-based eval and what requires a human to evaluate is fuzzy and is constantly shifting as models become more capable. We encourage experimentation to figure out how well model-based evals can work for your use case.

Tactic: Evaluate model outputs with reference to gold-standard answers
Suppose it is known that the correct answer to a question should make reference to a specific set of known facts. Then we can use a model query to count how many of the required facts are included in the answer.

For example, using the following system message:

SYSTEM
You will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:

- Neil Armstrong was the first person to walk on the moon.
- The date Neil Armstrong first walked on the moon was July 21, 1969.

For each of these points perform the following steps:

1 - Restate the point.
2 - Provide a citation from the answer which is closest to this point.
3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.
4 - Write "yes" if the answer to 3 was yes, otherwise write "no".

Finally, provide a count of how many "yes" answers there are. Provide this count as {"count": <insert count here>}.
Here's an example input where both points are satisfied:

SYSTEM
<insert system message above>
USER
"""Neil Armstrong is famous for being the first human to set foot on the Moon. This historic event took place on July 21, 1969, during the Apollo 11 mission."""
Here's an example input where only one point is satisfied:

SYSTEM
<insert system message above>
USER
"""Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon."""
Here's an example input where none are satisfied:

SYSTEM
<insert system message above>
USER
"""In the summer of '69, a voyage grand,
Apollo 11, bold as legend's hand.
Armstrong took a step, history unfurled,
"One small step," he said, for a new world."""
There are many possible variants on this type of model-based eval. Consider the following variation which tracks the kind of overlap between the candidate answer and the gold-standard answer, and also tracks whether the candidate answer contradicts any part of the gold-standard answer.

SYSTEM
Use the following steps to respond to user inputs. Fully restate each step before proceeding. i.e. "Step 1: Reason...".

Step 1: Reason step-by-step about whether the information in the submitted answer compared to the expert answer is either: disjoint, equal, a subset, a superset, or overlapping (i.e. some intersection but not subset/superset).

Step 2: Reason step-by-step about whether the submitted answer contradicts any aspect of the expert answer.

Step 3: Output a JSON object structured like: {"type_of_overlap": "disjoint" or "equal" or "subset" or "superset" or "overlapping", "contradiction": true or false}
Here's an example input with a substandard answer which nonetheless does not contradict the expert answer:

SYSTEM
<insert system message above>
USER
Question: """What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time."""

Submitted Answer: """Didn't he walk on the moon or something?"""

Expert Answer: """Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969."""
Here's an example input with answer that directly contradicts the expert answer:

SYSTEM
<insert system message above>
USER
Question: """What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time."""

Submitted Answer: """On the 21st of July 1969, Neil Armstrong became the second person to walk on the moon, following after Buzz Aldrin."""

Expert Answer: """Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969."""
Here's an example input with a correct answer that also provides a bit more detail than is necessary:

SYSTEM
<insert system message above>
USER
Question: """What event is Neil Armstrong most famous for and on what date did it occur? Assume UTC time."""

Submitted Answer: """At approximately 02:56 UTC on July 21st 1969, Neil Armstrong became the first human to set foot on the lunar surface, marking a monumental achievement in human history."""

Expert Answer: """Neil Armstrong is most famous for being the first person to walk on the moon. This historic event occurred on July 21, 1969."""

Brex's Prompt Engineering Guide
This guide was created by Brex for internal purposes. It's based on lessons learned from researching and creating Large Language Model (LLM) prompts for production use cases. It covers the history around LLMs as well as strategies, guidelines, and safety recommendations for working with and building programmatic systems on top of large language models, like OpenAI's GPT-4.

The examples in this document were generated with a non-deterministic language model and the same examples may give you different results.

This is a living document. The state-of-the-art best practices and strategies around LLMs are evolving rapidly every day. Discussion and suggestions for improvements are encouraged.

Table of Contents
What is a Large Language Model?
A Brief, Incomplete, and Somewhat Incorrect History of Language Models
Pre-2000‚Äôs
Mid-2000‚Äôs
Early-2010‚Äôs
Late-2010‚Äôs
2020‚Äôs
What is a prompt?
Hidden Prompts
Tokens
Token Limits
Prompt Hacking
Jailbreaks
Leaks
Why do we need prompt engineering?
Give a Bot a Fish
Semantic Search
Teach a Bot to Fish
Command Grammars
ReAct
GPT-4 vs GPT-3.5
Strategies
Embedding Data
Simple Lists
Markdown Tables
JSON
Freeform Text
Nested Data
Citations
Programmatic Consumption
Chain of Thought
Averaging
Interpreting Code
Delimiters
Fine Tuning
Downsides
Additional Resources
What is a Large Language Model (LLM)?
A large language model is a prediction engine that takes a sequence of words and tries to predict the most likely sequence to come after that sequence1. It does this by assigning a probability to likely next sequences and then samples from those to choose one2. The process repeats until some stopping criteria is met.

Large language models learn these probabilities by training on large corpuses of text. A consequence of this is that the models will cater to some use cases better than others (e.g. if it‚Äôs trained on GitHub data, it‚Äôll understand the probabilities of sequences in source code really well). Another consequence is that the model may generate statements that seem plausible, but are actually just random without being grounded in reality.

As language models become more accurate at predicting sequences, many surprising abilities emerge.

A Brief, Incomplete, and Somewhat Incorrect History of Language Models
üìå Skip to here if you'd like to jump past the history of language models. This section is for the curious minded, though may also help you understand the reasoning behind the advice that follows.

Pre-2000‚Äôs
Language models have existed for decades, though traditional language models (e.g. n-gram models) have many deficiencies in terms of an explosion of state space (the curse of dimensionality) and working with novel phrases that they‚Äôve never seen (sparsity). Plainly, older language models can generate text that vaguely resembles the statistics of human generated text, but there is no consistency within the output ‚Äì and a reader will quickly realize it‚Äôs all gibberish. N-gram models also don‚Äôt scale to large values of N, so are inherently limited.

Mid-2000‚Äôs
In 2007, Geoffrey Hinton ‚Äì famous for popularizing backpropagation in 1980‚Äôs ‚Äì published an important advancement in training neural networks that unlocked much deeper networks. Applying these simple deep neural networks to language modeling helped alleviate some of problems with language models ‚Äì they represented nuanced arbitrary concepts in a finite space and continuous way, gracefully handling sequences not seen in the training corpus. These simple neural networks learned the probabilities of their training corpus well, but the output would statistically match the training data and generally not be coherent relative to the input sequence.

Early-2010‚Äôs
Although they were first introduced in 1995, Long Short-Term Memory (LSTM) Networks found their time to shine in the 2010‚Äôs. LSTMs allowed models to process arbitrary length sequences and, importantly, alter their internal state dynamically as they processed the input to remember previous things they saw. This minor tweak led to remarkable improvements. In 2015, Andrej Karpathy famously wrote about creating a character-level lstm that performed far better than it had any right to.

LSTMs have seemingly magical abilities, but struggle with long term dependencies. If you asked it to complete the sentence, ‚ÄúIn France, we traveled around, ate many pastries, drank lots of wine, ... lots more text ... , but never learned how to speak _______‚Äù, the model might struggle with predicting ‚ÄúFrench‚Äù. They also process input one token at a time, so are inherently sequential, slow to train, and the Nth token only knows about the N - 1 tokens prior to it.

Late-2010‚Äôs
In 2017, Google wrote a paper, Attention Is All You Need, that introduced Transformer Networks and kicked off a massive revolution in natural language processing. Overnight, machines could suddenly do tasks like translating between languages nearly as good as (sometimes better than) humans. Transformers are highly parallelizable and introduce a mechanism, called ‚Äúattention‚Äù, for the model to efficiently place emphasis on specific parts of the input. Transformers analyze the entire input all at once, in parallel, choosing which parts are most important and influential. Every output token is influenced by every input token.

Transformers are highly parallelizable, efficient to train, and produce astounding results. A downside to transformers is that they have a fixed input and output size ‚Äì the context window ‚Äì and computation increases quadratically with the size of this window (in some cases, memory does as well!) 3.

Transformers are not the end of the road, but the vast majority of recent improvements in natural language processing have involved them. There is still abundant active research on various ways of implementing and applying them, such as Amazon‚Äôs AlexaTM 20B which outperforms GPT-3 in a number of tasks and is an order of magnitude smaller in its number of parameters.

2020‚Äôs
While technically starting in 2018, the theme of the 2020‚Äôs has been Generative Pre-Trained models ‚Äì more famously known as GPT. One year after the ‚ÄúAttention Is All You Need‚Äù paper, OpenAI released Improving Language Understanding by Generative Pre-Training. This paper established that you can train a large language model on a massive set of data without any specific agenda, and then once the model has learned the general aspects of language, you can fine-tune it for specific tasks and quickly get state-of-the-art results.

In 2020, OpenAI followed up with their GPT-3 paper Language Models are Few-Shot Learners, showing that if you scale up GPT-like models by another factor of ~10x, in terms of number of parameters and quantity of training data, you no longer have to fine-tune it for many tasks. The capabilities emerge naturally and you get state-of-the-art results via text interaction with the model.

In 2022, OpenAI followed-up on their GPT-3 accomplishments by releasing InstructGPT. The intent here was to tweak the model to follow instructions, while also being less toxic and biased in its outputs. The key ingredient here was Reinforcement Learning from Human Feedback (RLHF), a concept co-authored by Google and OpenAI in 20174, which allows humans to be in the training loop to fine-tune the model output to be more in line with human preferences. InstructGPT is the predecessor to the now famous ChatGPT.

OpenAI has been a major contributor to large language models over the last few years, including the most recent introduction of GPT-4, but they are not alone. Meta has introduced many open source large language models like OPT, OPT-IML (instruction tuned), and LLaMa. Google released models like FLAN-T5 and BERT. And there is a huge open source research community releasing models like BLOOM and StableLM.

Progress is now moving so swiftly that every few weeks the state-of-the-art is changing or models that previously required clusters to run now run on Raspberry PIs.

What is a prompt?
A prompt, sometimes referred to as context, is the text provided to a model before it begins generating output. It guides the model to explore a particular area of what it has learned so that the output is relevant to your goals. As an analogy, if you think of the language model as a source code interpreter, then a prompt is the source code to be interpreted. Somewhat amusingly, a language model will happily attempt to guess what source code will do:

The GPT-4 model interpreting Python code.

And it almost interprets the Python perfectly!

Frequently, prompts will be an instruction or a question, like:



On the other hand, if you don‚Äôt specify a prompt, the model has no anchor to work from and you‚Äôll see that it just randomly samples from anything it has ever consumed:

From GPT-3-Davinci:

image	image	image
From GPT-4:

image	image	image
Hidden Prompts
‚ö†Ô∏è Always assume that any content in a hidden prompt can be seen by the user.

In applications where a user is interacting with a model dynamically, such as chatting with the model, there will typically be portions of the prompt that are never intended to be seen by the user. These hidden portions may occur anywhere, though there is almost always a hidden prompt at the start of a conversation.

Typically, this includes an initial chunk of text that sets the tone, model constraints, and goals, along with other dynamic information that is specific to the particular session ‚Äì user name, location, time of day, etc...

The model is static and frozen at a point in time, so if you want it to know current information, like the time or the weather, you must provide it.

If you‚Äôre using the OpenAI Chat API, they delineate hidden prompt content by placing it in the system role.

Here‚Äôs an example of a hidden prompt followed by interactions with the content in that prompt:

 A very simple hidden prompt.

In this example, you can see we explain to the bot the various roles, some context on the user, some dynamic data we want the bot to have access to, and then guidance on how the bot should respond.

In practice, hidden prompts may be quite large. Here‚Äôs a larger prompt taken from a ChatGPT command-line assistant:

From: https://github.com/manno/chatgpt-linux-assistant
You‚Äôll see some good practices there, such as including lots of examples, repetition for important behavioral aspects, constraining the replies, etc‚Ä¶

‚ö†Ô∏è Always assume that any content in a hidden prompt can be seen by the user.

Tokens
If you thought tokens were üî• in 2022, tokens in 2023 are on a whole different plane of existence. The atomic unit of consumption for a language model is not a ‚Äúword‚Äù, but rather a ‚Äútoken‚Äù. You can kind of think of tokens as syllables, and on average they work out to about 750 words per 1,000 tokens. They represent many concepts beyond just alphabetical characters ‚Äì such as punctuation, sentence boundaries, and the end of a document.

Here‚Äôs an example of how GPT may tokenize a sequence:

An example tokenization. You can experiment here: https://platform.openai.com/tokenizer 

You can experiment with a tokenizer here: https://platform.openai.com/tokenizer

Different models will use different tokenizers with different levels of granularity. You could, in theory, just feed a model 0‚Äôs and 1‚Äôs ‚Äì but then the model needs to learn the concept of characters from bits, and then the concept of words from characters, and so forth. Similarly, you could feed the model a stream of raw characters, but then the model needs to learn the concept of words, and punctuation, etc‚Ä¶ and, in general, the models will perform worse.

To learn more, Hugging Face has a wonderful introduction to tokenizers and why they need to exist.

There‚Äôs a lot of nuance around tokenization, such as vocabulary size or different languages treating sentence structure meaningfully different (e.g. words not being separated by spaces). Fortunately, language model APIs will almost always take raw text as input and tokenize it behind the scenes ‚Äì so you rarely need to think about tokens.

Except for one important scenario, which we discuss next: token limits.

Token Limits
Prompts tend to be append-only, because you want the bot to have the entire context of previous messages in the conversation. Language models, in general, are stateless and won‚Äôt remember anything about previous requests to them, so you always need to include everything that it might need to know that is specific to the current session.

A major downside of this is that the leading language model architecture, the Transformer, has a fixed input and output size ‚Äì at a certain point the prompt can‚Äôt grow any larger. The total size of the prompt, sometimes referred to as the ‚Äúcontext window‚Äù, is model dependent. For GPT-3, it is 4,096 tokens. For GPT-4, it is 8,192 tokens or 32,768 tokens depending on which variant you use.

If your context grows too large for the model, the most common tactic is the truncate the context in a sliding window fashion. If you think of a prompt as hidden initialization prompt + messages[], usually the hidden prompt will remain unaltered, and the messages[] array will take the last N messages.

You may also see more clever tactics for prompt truncation ‚Äì such as discarding only the user messages first, so that the bot's previous answers stay in the context for as long as possible, or asking an LLM to summarize the conversation and then replacing all of the messages with a single message containing that summary. There is no correct answer here and the solution will depend on your application.

Importantly, when truncating the context, you must truncate aggressively enough to allow room for the response as well. OpenAI‚Äôs token limits include both the length of the input and the length of the output. If your input to GPT-3 is 4,090 tokens, it can only generate 6 tokens in response.

üßô‚Äç‚ôÇÔ∏è If you‚Äôd like to count the number of tokens before sending the raw text to the model, the specific tokenizer to use will depend on which model you are using. OpenAI has a library called tiktoken that you can use with their models ‚Äì though there is an important caveat that their internal tokenizer may vary slightly in count, and they may append other metadata, so consider this an approximation.

If you‚Äôd like an approximation without having access to a tokenizer, input.length / 4 will give a rough, but better than you‚Äôd expect, approximation for English inputs.

Prompt Hacking
Prompt engineering and large language models are a fairly nascent field, so new ways to hack around them are being discovered every day. The two large classes of attacks are:

Make the bot bypass any guidelines you have given it.
Make the bot output hidden context that you didn‚Äôt intend for the user to see.
There are no known mechanisms to comprehensively stop these, so it is important that you assume the bot may do or say anything when interacting with an adversarial user. Fortunately, in practice, these are mostly cosmetic concerns.

Think of prompts as a way to improve the normal user experience. We design prompts so that normal users don‚Äôt stumble outside of our intended interactions ‚Äì but always assume that a determined user will be able to bypass our prompt constraints.

Jailbreaks
Typically hidden prompts will tell the bot to behave with a certain persona and focus on specific tasks or avoid certain words. It is generally safe to assume the bot will follow these guidelines for non-adversarial users, although non-adversarial users may accidentally bypass the guidelines too.

For example, we can tell the bot:

You are a helpful assistant, but you are never allowed to use the word "computer".
If we then ask it a question about computers, it will refer to them as a ‚Äúdevice used for computing‚Äù because it isn‚Äôt allowed to use the word ‚Äúcomputer‚Äù.

GPT-4 trying hard to not say the word 'computer'.

It will absolutely refuse to say the word:



But we can bypass these instructions and get the model to happily use the word if we trick it by asking it to translate the pig latin version of ‚Äúcomputer‚Äù.



There are a number of defensive measures you can take here, but typically the best bet is to reiterate your most important constraints as close to the end as possible. For the OpenAI chat API, this might mean including it as a system message after the last user message. Here‚Äôs an example:

image	image
Despite OpenAI investing a lot into jailbreaks, there are very clever work arounds being shared every day.

Leaks
If you missed the previous warnings in this doc, you should always assume that any data exposed to the language model will eventually be seen by the user.

As part of constructing prompts, you will often embed a bunch of data in hidden prompts (a.k.a. system prompts). The bot will happily relay this information to the user:

The bot happily regurgitating the information it knows about the user.

Even if you instruct it not to reveal the information, and it obeys those instructions, there are millions of ways to leak data in the hidden prompt.

Here we have an example where the bot should never mention my city, but a simple reframing of the question get‚Äôs it to spill the beans.

The bot refuses to reveal personal information, but we convince it to tell me what city I‚Äôm in regardless.

Similarly, we get the bot to tell us what word it isn‚Äôt allowed to say without ever actually saying the word:

Technically, the bot never said 'computer', but I was still able to get it to tell me everything I needed to know about it.

You should think of a hidden prompt as a means to make the user experience better or more inline with the persona you‚Äôre targeting. Never place any information in a prompt that you wouldn‚Äôt visually render for someone to read on screen.

Why do we need prompt engineering?
Up above, we used an analogy of prompts as the ‚Äúsource code‚Äù that a language model ‚Äúinterprets‚Äù. Prompt engineering is the art of writing prompts to get the language model to do what we want it to do ‚Äì just like software engineering is the art of writing source code to get computers to do what we want them to do.

When writing good prompts, you have to account for the idiosyncrasies of the model(s) you‚Äôre working with. The strategies will vary with the complexity of the tasks. You‚Äôll have to come up with mechanisms to constrain the model to achieve reliable results, incorporate dynamic data that the model can‚Äôt be trained on, account for limitations in the model‚Äôs training data, design around context limits, and many other dimensions.

There‚Äôs an old adage that computers will only do what you tell them to do. Throw that advice out the window. Prompt engineering inverts this wisdom. It‚Äôs like programming in natural language against a non-deterministic computer that will do anything that you haven‚Äôt guided it away from doing.

There are two broad buckets that prompt engineering approaches fall into.

Give a Bot a Fish
The ‚Äúgive a bot a fish‚Äù bucket is for scenarios when you can explicitly give the bot, in the hidden context, all of the information it needs to do whatever task is requested of it.

For example, if a user loaded up their dashboard and we wanted to show them a quick little friendly message about what task items they have outstanding, we could get the bot to summarize it as

You have 4 receipts/memos to upload. The most recent is from Target on March 5th, and the oldest is from Blink Fitness on January 17th. Thanks for staying on top of your expenses!

by providing a list of the entire inbox and any other user context we‚Äôd like it to have.

GPT-3 summarizing a task inbox.

Similarly, if you were helping a user book a trip, you could:

Ask the user their dates and destination.
Behind the scenes, search for flights and hotels.
Embed the flight and hotel search results in the hidden context.
Also embed the company‚Äôs travel policy in the hidden context.
And then the bot will have real-time travel information + constraints that it can use to answer questions for the user. Here‚Äôs an example of the bot recommending options, and the user asking it to refine them:

GPT-4 helping a user book a trip.

(Full prompt)
This is the same approach that products like Microsoft Bing use to incorporate dynamic data. When you chat with Bing, it asks the bot to generate three search queries. Then they run three web searches and include the summarized results in the hidden context for the bot to use.

Summarizing this section, the trick to making a good experience is to change the context dynamically in response to whatever the user is trying to do.

üßô‚Äç‚ôÇÔ∏è Giving a bot a fish is the most reliable way to ensure the bot gets a fish. You will get the most consistent and reliable results with this strategy. Use this whenever you can.

Semantic Search
If you just need the bot to know a little more about the world, a common approach is to perform a semantic search.

A semantic search is oriented around a document embedding ‚Äì which you can think of as a fixed-length array5 of numbers, where each number represents some aspect of the document (e.g. if it‚Äôs a science document, maybe the 843rd number is large, but if it‚Äôs an art document the 1,115th number is large ‚Äì this is overly simplistic, but conveys the idea).6

In addition to computing an embedding for a document, you can also compute an embedding for a user query using the same function. If the user asks ‚ÄúWhy is the sky blue?‚Äù ‚Äì you compute the embedding of that question and, in theory, this embedding will be more similar to embeddings of documents that mention the sky than embeddings that don‚Äôt talk about the sky.

To find documents related to the user query, you compute the embedding and then find the top-N documents that have the most similar embedding. Then we place these documents (or summaries of these documents) in the hidden context for the bot to reference.

Notably, sometimes user queries are so short that the embedding isn‚Äôt particularly valuable. There is a clever technique described in a paper published in December 2022 called a ‚ÄúHypothetical Document Embedding‚Äù or HyDE. Using this technique, you ask the model to generate a hypothetical document in response to the user‚Äôs query, and then compute the embedding for this generated document. The model fabricates a document out of thin air ‚Äì but the approach works!

The HyDE technique uses more calls to the model, but for many use cases has notable boosts in results.

Teach a Bot to Fish
Sometimes you‚Äôll want the bot to have the capability to perform actions on the user‚Äôs behalf, like adding a memo to a receipt or plotting a chart. Or perhaps we want it to retrieve data in more nuanced ways than semantic search would allow for, like retrieving the past 90 days of expenses.

In these scenarios, we need to teach the bot how to fish.

Command Grammars
We can give the bot a list of commands for our system to interpret, along with descriptions and examples for the commands, and then have it produce programs composed of those commands.

There are many caveats to consider when going with this approach. With complex command grammars, the bot will tend to hallucinate commands or arguments that could plausibly exist, but don‚Äôt actually. The art to getting this right is enumerating commands that have relatively high levels of abstraction, while giving the bot sufficient flexibility to compose them in novel and useful ways.

For example, giving the bot a plot-the-last-90-days-of-expenses command is not particularly flexible or composable in what the bot can do with it. Similarly, a draw-pixel-at-x-y [x] [y] [rgb] command would be far too low-level. But giving the bot a plot-expenses and list-expenses command provides some good primitives that the bot has some flexibility with.

In an example below, we use this list of commands:

Command	Arguments	Description
list-expenses	budget	Returns a list of expenses for a given budget
converse	message	A message to show to the user
plot-expenses	expenses[]	Plots a list of expenses
get-budget-by-name	budget_name	Retrieves a budget by name
list-budgets		Returns a list of budgets the user has access to
add-memo	inbox_item_id, memo message	Adds a memo to the provided inbox item
We provide this table to the model in Markdown format, which the language model handles incredibly well ‚Äì presumably because OpenAI trains heavily on data from GitHub.

In this example below, we ask the model to output the commands in reverse polish notation7.

A bot happily generating commands to run in response to user queries.

üß† There are some interesting subtle things going on in that example, beyond just command generation. When we ask it to add a memo to the ‚Äúshake shack‚Äù expense, the model knows that the command add-memo takes an expense ID. But we never tell it the expense ID, so it looks up ‚ÄúShake Shack‚Äù in the table of expenses we provided it, then grabs the ID from the corresponding ID column, and then uses that as an argument to add-memo.

Getting command grammars working reliably in complex situations can be tricky. The best levers we have here are to provide lots of descriptions, and as many examples of usage as we can. Large language models are few-shot learners, meaning that they can learn a new task by being provided just a few examples. In general, the more examples you provide the better off you‚Äôll be ‚Äì but that also eats into your token budget, so it‚Äôs a balance.

Here‚Äôs a more complex example, with the output specified in JSON instead of RPN. And we use Typescript to define the return types of commands.

A bot happily generating commands to run in response to user queries.

(Full prompt)
This version is a bit easier to parse and interpret if your language of choice has a JSON.parse function.

üßô‚Äç‚ôÇÔ∏è There is no industry established best format for defining a DSL for the model to generate programs. So consider this an area of active research. You will bump into limits. And as we overcome these limits, we may discover more optimal ways of defining commands.

ReAct
In March of 2023, Princeton and Google released a paper ‚ÄúReAct: Synergizing Reasoning and Acting in Language Models‚Äù, where they introduce a variant of command grammars that allows for fully autonomous interactive execution of actions and retrieval of data.

The model is instructed to return a thought and an action that it would like to perform. Another agent (e.g. our client) then performs the action and returns it to the model as an observation. The model will then loop to return more thoughts and actions until it returns an answer.

This is an incredibly powerful technique, effectively allowing the bot to be its own research assistant and possibly take actions on behalf of the user. Combined with a powerful command grammar, the bot should rapidly be able to answer a massive set of user requests.

In this example, we give the model a small set of commands related to getting employee data and searching wikipedia:

Command	Arguments	Description
find_employee	name	Retrieves an employee by name
get_employee	id	Retrieves an employee by ID
get_location	id	Retrieves a location by ID
get_reports	employee_id	Retrieves a list of employee ids that report to the employee associated with employee_id.
wikipedia	article	Retrieves a wikipedia article on a topic.
We then ask the bot a simple question, ‚ÄúIs my manager famous?‚Äù.

We see that the bot:

First looks up our employee profile.
From our profile, gets our manager‚Äôs id and looks up their profile.
Extracts our manager‚Äôs name and searches for them on Wikipedia.
I chose a fictional character for the manager in this scenario.
The bot reads the wikipedia article and concludes that can‚Äôt be my manager since it is a fictional character.
The bot then modifies its search to include (real person).
Seeing that there are no results, the bot concludes that my manager is not famous.
image	image
(Full prompt)
GPT-4 vs GPT-3.5
In most of the examples in this doc, the difference between GPT-3.5 and GPT-4 is negligible, but for ‚Äúteaching a bot to fish‚Äù scenarios the difference between the models is notable.

None of the above examples of command grammars, for example, work without meaningful modifications for GPT-3.5. At a minimum, you have to provide a number of examples (at least one usage example per command) before you get any reasonable results. And, for complex sets of commands, it may hallucinate new commands or create fictional arguments.

With a sufficiently thorough hidden prompt, you should be able to overcome these limitations. GPT-4 is capable of far more consistent and complex logic with far simpler prompts (and can get by with zero or small numbers of examples ‚Äì though it is always beneficial to include as many as possible).

Strategies
This section contains examples and strategies for specific needs or problems. For successful prompt engineering, you will need to combine some subset of all of the strategies enumerated in this document. Don‚Äôt be afraid to mix and match things ‚Äì or invent your own approaches.

Embedding Data
In hidden contexts, you‚Äôll frequently want to embed all sorts of data. The specific strategy will vary depending on the type and quantity of data you are embedding.

Simple Lists
For one-off objects, enumerating fields + values in a normal bulleted list works pretty well:

GPT-4 extracting Steve‚Äôs occupation from a list attributes.

It will also work for larger sets of things, but there are other formats for lists of data that GPT handles more reliably. Regardless, here‚Äôs an example:

GPT-4 answering questions about a set of expenses.

Markdown Tables
Markdown tables are great for scenarios where you have many items of the same type to enumerate.

Fortunately, OpenAI‚Äôs models are exceptionally good at working with Markdown tables (presumably from the tons of GitHub data they‚Äôve trained on).

We can reframe the above using Markdown tables instead:

GPT-4 answering questions about a set of expenses from a Markdown table.

GPT-4 answering questions about a set of expenses from a Markdown table.

üß† Note that in this last example, the items in the table have an explicit date, February 2nd. In our question, we asked about ‚Äútoday‚Äù. And earlier in the prompt we mentioned that today was Feb 2. The model correctly handled the transitive inference ‚Äì converting ‚Äútoday‚Äù to ‚ÄúFebruary 2nd‚Äù and then looking up ‚ÄúFebruary 2nd‚Äù in the table.

JSON
Markdown tables work really well for many use cases and should be preferred due to their density and ability for the model to handle them reliably, but you may run into scenarios where you have many columns and the model struggles with it or every item has some custom attributes and it doesn‚Äôt make sense to have dozens of columns of empty data.

In these scenarios, JSON is another format that the model handles really well. The close proximity of keys to their values makes it easy for the model to keep the mapping straight.

Here is the same example from the Markdown table, but with JSON instead:

GPT-4 answering questions about a set of expenses from a JSON blob.

Freeform Text
Occasionally you‚Äôll want to include freeform text in a prompt that you would like to delineate from the rest of the prompt ‚Äì such as embedding a document for the bot to reference. In these scenarios, surrounding the document with triple backticks, ```, works well8.

GPT-4 answering questions about a set of expenses from a JSON blob.

Nested Data
Not all data is flat and linear. Sometimes you‚Äôll need to embed data that is nested or has relations to other data. In these scenarios, lean on JSON:

GPT-4 handles nested JSON very reliably.

(Full prompt)
If using nested JSON winds up being too verbose for your token budget, fallback to relational tables defined with Markdown:

GPT-4 handles relational tables pretty reliably too.

(Full prompt)
üß† The model works well with data in 3rd normal form, but may struggle with too many joins. In experiments, it seems to do okay with at least three levels of nested joins. In the example above the model successfully joins from users to addresses to cities to infer the likely income for George ‚Äì $90,000.

Citations
Frequently, a natural language response isn‚Äôt sufficient on its own and you‚Äôll want the model‚Äôs output to cite where it is getting data from.

One useful thing to note here is that anything you might want to cite should have a unique ID. The simplest approach is to just ask the model to link to anything it references:

GPT-4 will reliably link to data if you ask it to.

Programmatic Consumption
By default, language models output natural language text, but frequently we need to interact with this result in a programmatic way that goes beyond simply printing it out on screen. You can achieve this by asking the model to output the results in your favorite serialization format (JSON and YAML seem to work best).

Make sure you give the model an example of the output format you‚Äôd like. Building on our previous travel example above, we can augment our prompt to tell it:

Produce your output as JSON. The format should be:
```
{
    message: "The message to show the user",
    hotelId: 432,
    flightId: 831
}
```

Do not include the IDs in your message.
And now we‚Äôll get interactions like this:

GPT-4 providing travel recommendations in an easy to work with format.

You could imagine the UI for this rendering the message as normal text, but then also adding discrete buttons for booking the flight + hotel, or auto-filling a form for the user.

As another example, let‚Äôs build on the citations example ‚Äì but move beyond Markdown links. We can ask it to produce JSON with a normal message along with a list of items used in the creation of that message. In this scenario you won‚Äôt know exactly where in the message the citations were leveraged, but you‚Äôll know that they were used somewhere.

Asking the model to provide a list of citations is a reliable way to programmatically know what data the model leaned on in its response.

üß† Interestingly, in the model‚Äôs response to ‚ÄúHow much did I spend at Target?‚Äù it provides a single value, $188.16, but importantly in the citations array it lists the individual expenses that it used to compute that value.

Chain of Thought
Sometimes you will bang your head on a prompt trying to get the model to output reliable results, but, no matter what you do, it just won‚Äôt work. This will frequently happen when the bot‚Äôs final output requires intermediate thinking, but you ask the bot only for the output and nothing else.

The answer may surprise you: ask the bot to show its work. In October 2022, Google released a paper ‚ÄúChain-of-Thought Prompting Elicits Reasoning in Large Language Models‚Äù where they showed that if, in your hidden prompt, you give the bot examples of answering questions by showing your work, then when you ask the bot to answer something it will show its work and produce more reliable answers.

Just a few weeks after that paper was published, at the end of October 2022, the University of Tokyo and Google released the paper ‚ÄúLarge Language Models are Zero-Shot Reasoners‚Äù, where they show that you don‚Äôt even need to provide examples ‚Äì you simply have to ask the bot to think step-by-step.

Averaging
Here is an example where we ask the bot to compute the average expense, excluding Target. The actual answer is $136.77 and the bot almost gets it correct with $136.43.

The model **almost** gets the average correct, but is a few cents off.

If we simply add ‚ÄúLet‚Äôs think step-by-step‚Äù, the model gets the correct answer:

When we ask the model to show its work, it gets the correct answer.

Interpreting Code
Let‚Äôs revisit the Python example from earlier and apply chain-of-thought prompting to our question. As a reminder, when we asked the bot to evaluate the Python code it gets it slightly wrong. The correct answer is Hello, Brex!!Brex!!Brex!!! but the bot gets confused about the number of !'s to include. In below‚Äôs example, it outputs Hello, Brex!!!Brex!!!Brex!!!:

The bot almost interprets the Python code correctly, but is a little off.

If we ask the bot to show its work, then it gets the correct answer:

The bot correctly interprets the Python code if you ask it to show its work.

Delimiters
In many scenarios, you may not want to show the end user all of the bot‚Äôs thinking and instead just want to show the final answer. You can ask the bot to delineate the final answer from its thinking. There are many ways to do this, but let‚Äôs use JSON to make it easy to parse:

The bot showing its work while also delimiting the final answer for easy extraction.

Using Chain-of-Thought prompting will consume more tokens, resulting in increased price and latency, but the results are noticeably more reliable for many scenarios. It‚Äôs a valuable tool to use when you need the bot to do something complex and as reliably as possible.

Prompt Engineering
Date: March 15, 2023 | Estimated Reading Time: 21 min | Author: Lilian Weng
Table of Contents
Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.

This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.

[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.

Basic Prompting
Zero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.

Zero-Shot
Zero-shot learning is to simply feed the task text to the model and ask for results.

(All the sentiment analysis examples are from SST-2)

Text: i'll bet the video game is a lot more fun than the film.
Sentiment:
Few-shot
Few-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.

Text: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.
Sentiment: positive

Text: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.
Sentiment: negative

Text: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.
Sentiment: positive

Text: i'll bet the video game is a lot more fun than the film.
Sentiment:
Many studies looked into how to construct in-context examples to maximize the performance and observed that choice of prompt format, training examples, and the order of the examples can lead to dramatically different performance, from near random guess to near SoTA.

Zhao et al. (2021) investigated the case of few-shot classification and proposed that several biases with LLM (they use GPT-3 in the experiments) contribute to such high variance: (1) Majority label bias exists if distribution of labels among the examples is unbalanced; (2) Recency bias refers to the tendency where the model may repeat the label at the end; (3) Common token bias indicates that LLM tends to produce common tokens more often than rare tokens. To conquer such bias, they proposed a method to calibrate the label probabilities output by the model to be uniform when the input string is N/A.

Tips for Example Selection
Choose examples that are semantically similar to the test example using 
-NN clustering in the embedding space (Liu et al., 2021)

To select a diverse and representative set of examples, Su et al. (2022) proposed to use a graph-based approach: (1) First, construct a directed graph 
 based on the embedding (e.g. by SBERT or other embedding models) cosine similarity between samples, where each node points to its 
 nearest neighbors; (2) Start with a set of selected samples 
 and a set of remaining samples 
. Each sample 
 is scored by
 
such that 
 is low if many of 
‚Äôs neighbors are selected and thus the scoring encourages to pick diverse samples.

Rubin et al. (2022) proposed to train embeddings via contrastive learning specific to one training dataset for in-context learning sample selection. Given each training pair 
, the quality of one example 
 (formatted input-output pair) can be measured by a conditioned probability assigned by LM: 
. We can identify other examples with top-
 and bottom-
 scores as positive and negative sets of candidates for every training pair and use that for contrastive learning.

Some researchers tried Q-Learning to do sample selection. (Zhang et al. 2022)

Motivated by uncertainty-based active learning, Diao et al. (2023) suggested to identify examples with high disagreement or entropy among multiple sampling trials. Then annotate these examples to be used in few-shot prompts.

Tips for Example Ordering
A general suggestion is to keep the selection of examples diverse, relevant to the test sample and in random order to avoid majority label bias and recency bias.
Increasing model sizes or including more training examples does not reduce variance among different permutations of in-context examples. Same order may work well for one model but badly for another. When the validation set is limited, consider choosing the order such that the model does not produce extremely unbalanced predictions or being overconfident about its predictions. (Lu et al. 2022)
Instruction Prompting
The purpose of presenting few-shot examples in the prompt is to explain our intent to the model; in other words, describe the task instruction to the model in the form of demonstrations. However, few-shot can be expensive in terms of token usage and restricts the input length due to limited context length. So, why not just give the instruction directly?

Instructed LM (e.g. InstructGPT, natural instruction) finetunes a pretrained model with high-quality tuples of (task instruction, input, ground truth output) to make LM better understand user intention and follow instruction. RLHF (Reinforcement Learning from Human Feedback) is a common method to do so. The benefit of instruction following style fine-tuning improves the model to be more aligned with human intention and greatly reduces the cost of communication.

When interacting with instruction models, we should describe the task requirement in details, trying to be specific and precise and avoiding say ‚Äúnot do something‚Äù but rather specify what to do.

Please label the sentiment towards the movie of the given movie review. The sentiment label should be "positive" or "negative". 
Text: i'll bet the video game is a lot more fun than the film. 
Sentiment:
Explaining the desired audience is another smart way to give instructions

For example to produce education materials for kids,
Describe what is quantum physics to a 6-year-old.
And safe content,
... in language that is safe for work.
In-context instruction learning (Ye et al. 2023) combines few-shot learning with instruction prompting. It incorporates multiple demonstration examples across different tasks in the prompt, each demonstration consisting of instruction, task input and output. Note that their experiments were only on classification tasks and the instruction prompt contains all label options.

Definition: Determine the speaker of the dialogue, "agent" or "customer".
Input: I have successfully booked your tickets.
Ouput: agent

Definition: Determine which category the question asks for, "Quantity" or "Location".
Input: What's the oldest building in US?
Ouput: Location

Definition: Classify the sentiment of the given movie review, "positive" or "negative".
Input: i'll bet the video game is a lot more fun than the film.
Output:
Self-Consistency Sampling
Self-consistency sampling (Wang et al. 2022a) is to sample multiple outputs with temperature > 0 and then selecting the best one out of these candidates. The criteria for selecting the best candidate can vary from task to task. A general solution is to pick majority vote. For tasks that are easy to validate such as a programming question with unit tests, we can simply run through the interpreter and verify the correctness with unit tests.

Chain-of-Thought (CoT)
Chain-of-thought (CoT) prompting (Wei et al. 2022) generates a sequence of short sentences to describe reasoning logics step by step, known as reasoning chains or rationales, to eventually lead to the final answer. The benefit of CoT is more pronounced for complicated reasoning tasks, while using large models (e.g. with more than 50B parameters). Simple tasks only benefit slightly from CoT prompting.

Types of CoT prompts
Two main types of CoT prompting:

Few-shot CoT. It is to prompt the model with a few demonstrations, each containing manually written (or model-generated) high-quality reasoning chains.
(All the math reasoning examples are from GSM8k)

Question: Tom and Elizabeth have a competition to climb a hill. Elizabeth takes 30 minutes to climb the hill. Tom takes four times as long as Elizabeth does to climb the hill. How many hours does it take Tom to climb up the hill?
Answer: It takes Tom 30*4 = <<30*4=120>>120 minutes to climb the hill.
It takes Tom 120/60 = <<120/60=2>>2 hours to climb the hill.
So the answer is 2.
===
Question: Jack is a soccer player. He needs to buy two pairs of socks and a pair of soccer shoes. Each pair of socks cost $9.50, and the shoes cost $92. Jack has $40. How much more money does Jack need?
Answer: The total cost of two pairs of socks is $9.50 x 2 = $<<9.5*2=19>>19.
The total cost of the socks and the shoes is $19 + $92 = $<<19+92=111>>111.
Jack need $111 - $40 = $<<111-40=71>>71 more.
So the answer is 71.
===
Question: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?
Answer:
Zero-shot CoT. Use natural language statement like Let's think step by step to explicitly encourage the model to first generate reasoning chains and then to prompt with Therefore, the answer is to produce answers (Kojima et al. 2022 ). Or a similar statement Let's work this out it a step by step to be sure we have the right answer (Zhou et al. 2022).
Question: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?
Answer: Let's think step by step.
Tips and Extensions
Self-consistency sampling can improve reasoning accuracy by sampling a number of diverse answers and then taking the majority vote. (Wang et al. 2022a)

Another approach for ensemble learning is to alter the example order or use model generated rationales to replace human-written ones to introduce randomness during multiple sample trials. Then aggregate model outputs with a majority vote to get final answer. (Wang et al. 2022b)

If training examples are only associated with true answers (easy to verify!) but no rationales, we can follow the STaR (Self-Taught Reasoner; Zelikman et al. 2022) method : (1) Ask LLM to generate reasoning chains and only keep those leading to correct answers; (2) Then fine-tune the model with generated rationales and repeat the process until convergence. Note that higher temperature is more likely to generate incorrect rationales with correct answers. If training examples do not have ground truth answers, maybe consider using majority votes as the ‚Äúcorrect‚Äù answers.

Prompts with demonstrations of higher reasoning complexity can achieve better performance, where complexity is measured by the number of reasoning steps in the chains. When separating reasoning steps, newline \n symbol works better than step i, period . or semicolon ;. (Fu et al. 2023)

Complexity-based consistency is to explicitly prefer complex chains among all the generations by taking majority vote among only top 
 complex chains. (Fu et al. 2023)

Later, Shum et al. (2023) found that in their experiments CoT prompts with only complex examples can improve the accuracy of complex questions, but perform poorly in simple questions; evidence shown on GSM8k.

Changing Q: to Question: is found to be helpful. (Fu et al. 2023)

Ye & Durrett (2022) found that the benefit of including explanations in the prompt is small to moderate for NLP tasks that involve reasoning over text (i.e. QA and NLI) and the effects vary by models. They observed that explanations are more likely to be nonfactual than be inconsistent (i.e. whether explanation entails prediction). Nonfactual explanations most likely lead to incorrect predictions.

Self-Ask (Press et al. 2022) is a method to repeatedly prompt the model to ask following-up questions to construct the thought process iteratively. Follow-up questions can be answered by search engine results. Similarly, IRCoT (Interleaving Retrieval CoT; Trivedi et al. 2022) and ReAct (Reason + Act; Yao et al. 2023) combines iterative CoT prompting with queries to Wikipedia APIs to search for relevant entities and content and then add it back into the context.


Fig. 1. How Self-Ask works with external search queries.
(Image source: Press et al. 2022).
Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, essentially creating a tree structure. The search process can be BFS or DFS while each state is evaluated by a classifier (via a prompt) or majority vote.

Fig. 2. How Self-Ask works with external search queries.
(Image source: Yao et al. 2022).
Automatic Prompt Design
Prompt is a sequence of prefix tokens that increase the probability of getting desired output given input. Therefore we can treat them as trainable parameters and optimize them directly on the embedding space via gradient descent, such as AutoPrompt (Shin et al., 2020, Prefix-Tuning (Li & Liang (2021)), P-tuning (Liu et al. 2021) and Prompt-Tuning (Lester et al. 2021). This section in my ‚ÄúControllable Neural Text Generation‚Äù post has a good coverage of them. The trend from AutoPrompt to Prompt-Tuning is that the setup gets gradually simplified.

APE (Automatic Prompt Engineer; Zhou et al. 2022) is a method to search over a pool of model-generated instruction candidates and then filters the candidate set according to a chosen score function to ultimately choose the best candidate with highest score.

Prompt LLM to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. E.g. {{Given desired input-output pairs}}\n\nThe instruction is.

Given a dataset of 
, we would like to find an instruction 
 such that 
, where 
 is a per-sample score function, such as execution accuracy ùüô
 or log probability: 
.

Use an iterative Monte Carlo search method to improve the best candidates by proposing semantically similar variants via prompts like Generate a variation of the following instruction while keeping the semantic meaning.\n\nInput: ...\n\nOutput:...

To construct chain-of-thought prompts automatically, Shum et al. (2023) suggested augment-prune-select, a three-step process:

Augment: Generate multiple pseudo-chains of thought given question using few-shot or zero-shot CoT prompts;
Prune: Prune pseudo chains based on whether generated answers match ground truths.
Select: Apply a variance-reduced policy gradient strategy to learn the probability distribution over selected examples, while considering the probability distribution over examples as policy and the validation set accuracy as reward.
Zhang et al. (2023) instead adopted clustering techniques to sample questions and then generates chains. They observed that LLMs tend to make certain types of mistakes. One type of errors can be similar in the emebedding space and thus get grouped together. By only sampling one or a few from frequent-error clusters, we can prevent too many wrong demonstrations of one error type and collect a diverse set of examples.

Question clustering: Embed questions and run 
-means for clustering.
Demonstration selection: Select a set of representative questions from each cluster; i.e. one demonstration from one cluster. Samples in each cluster are sorted by distance to the cluster centroid and those closer to the centroid are selected first.
Rationale generation: Use zero-shot CoT to generate reasoning chains for selected questions and construct few-shot prompt to run inference.
Augmented Language Models
A survey on augmented language models by Mialon et al. (2023) has great coverage over multiple categories of language models augmented with reasoning skills and the ability of using external tools. Recommend it.

Retrieval
Often we need to complete tasks that require latest knowledge after the model pretraining time cutoff or internal/private knowledge base. In that case, the model would not know the context if we don‚Äôt explicitly provide it in the prompt. Many methods for Open Domain Question Answering depend on first doing retrieval over a knowledge base and then incorporating the retrieved content as part of the prompt. The accuracy of such a process depends on the quality of both retrieval and generation steps.

Lazaridou et al. (2022) studied how to use Google Search for document retrieval to augment LLMs. Given a question 
, clean text is extracted out of 20 URLs returned by Google, resulting in a set of documents. Because these documents are long, each document is split into paragraphs of 6 sentences, 
. Paragraphs are ranked by TF-IDF based cosine similarity between evidence paragraphs and the query. Only the most relevant paragraph is used in the prompt to produce an answer 
.

For closed-book QA, each demonstration is formatted as follows to construct few-shot prompts. Swapping the question with the evidence (longer distance between questions and answers) is found to consistently yield lower results across all datasets.

Evidence: ...
Question: ...
Answer: ...
The answer probability is computed in three ways:

RAG style, 
, where 
 is the normalized cosine similarities between the TF-IDF passage and question representations.
Noisy channel inference, 
 
Product-of-Experts (PoE), combines all probabilities used above in addition to 
.
According to their experiments on generation and classification tasks, among three answer reranking scores - PoE > Noisy channel > RAG. Among individual probabilities, 
 and 
 are found to be most informative. 
 captures how well the question can be explained by LM given evidence paragraph and answer and can reliably be used for reranking answer candidates.

One observation with SituatedQA dataset for questions grounded in different dates is that despite LM (pretraining cutoff is year 2020) has access to latest information via Google Search, its performance on post-2020 questions are still a lot worse than on pre-2020 questions. This suggests the existence of some discrepencies or conflicting parametric between contextual information and model internal knowledge.

Interestingly it is found to be beneficial even with only ‚Äúinternal retrieval‚Äù, that is, to generate knowledge about a topic before answering the question (Liu et al. 2022). First we can use the following template to extract knowledge:

Generate some knowledge about the input. Examples:

Input: What type of water formation is formed by clouds?
Knowledge: Clouds are made of water vapor.

Input: {question}
Knowledge:
And then with model-generated knowledge, prompt the LM further to get the answer.

Programming Language
Both PAL (Program-aided language models); Gao et al. 2022) and PoT (Program of Thoughts prompting; Chen et al. 2022) ask LLM to generate programming language statements to resolve natural language reasoning problems, hence offloading the solution step to a runtime such as a Python interpreter. Such setup decouples complex computation and reasoning. It relies on a LM with good enough coding skills.


Fig. 3. Comparing CoT and PoT. (Image source: Chen et al. 2022).
External APIs
TALM (Tool Augmented Language Models; Parisi et al. 2022) is a language model augmented with text-to-text API calls. LM is guided to generate |tool-call and tool input text conditioned on task input text to construct API call requests. When |result shows up, the specified tool API is called and the returned result gets appended to the text sequence. The final output is generated following |output token.


Fig. 4. The format of API calls in TALM. (Image source: Parisi et al. 2022).
TALM adopts a self-play approach to iteratively bootstrap the dataset of tool use examples and finetune LM with it. This self-play, defined as a model interacting with a tool API, iteratively expands the dataset based on whether a newly added tool API can improve the model outputs. Same idea is adopted in Toolformer too, described in more details below. The pipeline loosely mimics a RL process where LM is the policy network and it is trained by policy gradient with a binary reward signal.


Fig. 5. Self-play iterations help boost the model performance.
(Image source: Parisi et al. 2022).
Toolformer (Schick et al. 2023) is a LM that can use external tools via simple APIs, which is built in a self-supervised manner and only requires a handful of demonstrations for each API. The toolbox of Toolformer includes:

Calculator to help LM with the lack of precise math skills;
Q&A system to help with unfaithful content and hallucination;
Search engine to provide up-to-date information after pretraining cut off time;
Translation system to improve performance on low resource language;
Calendar to make LM be aware of time progression.

Fig. 6. Illustration of how to build Toolformer.
(Image source: Schick et al. 2023).
Toolformer is trained as follows:

Prompting to annotate potential API calls. Ask a pre-trained LM to annotate a dataset via few-shot learning with API call usage examples. Formatting example:


Fig. 7. How dataset is annotated to do API calls.
(Image source: Schick et al. 2023).
Each API call is represented as a tuple of (API name, corresponding input), 
 and its corresponding result is denoted as 
. The API call sequences with and without results are labeled as follows, respectively:

 
Sample API calls based on the probabilities 
 and select top 
 candidate positions for doing API calls at position 
 if the probability is larger than a threshold.

Then we sample potential API calls from the LM given the sequence 
 as prefix and 
 as suffix.

Filter annotations based on whether API calls help model predict future tokens. Use a self-supervised loss to decide which API calls are actually helpful.

Execute each API call 
 to get corresponding result 
.

Compute weighted cross entropy loss for the LM over tokens 
 when the model is prefixed with the prompt. Two versions are computed, one with API result and the other with empty sequence 
.

 
Only API calls with 
 larger than a threshold are kept, meaning that adding this API call and its results help the model predict future tokens.

Fine-tune LM on this annotated dataset. The new training sequences are constructed as 
 . The training data is a combination of the original dataset (e.g. a subset of CCNet, as in the paper) and its augmented version.

At inference time, decoding runs until the model produces ‚Äú
 " token, indicating that it is expecting response from an API call next.

Toolformer currently does not support tool use in a chain (i.e. using the output of one tool as an input for another tool) or in an interactive way (i.e. adopt API response after human selection). Both are interesting future directions to expand the model for.

Six months ago I published my Prompt Engineering 101 post. That later turned into a pretty popular LinkedIn course that has been taken by over 20k people at this point. That post and course were thought out as a gentle introduction to the topic. If you are new to prompt engineering, please start there. Also, a lot has happened in the world of LLMs since then. So, now is a good time to complement the Prompt Engineering 101 with an up-to-date and a bit more advanced post. I‚Äôll call it Prompt Engineering 201. I will start off by repeating a ‚Äúclassic‚Äù technique that was already covered in the ‚Äúadvanced section‚Äù of the original post, Chain of Thought, but will build up from there.

Before we get into those techniques, a few words on what is Prompt Engineering.

Prompt Design and EngineeringPermalink
Prompt Design is the process of coming up with the optimal prompt given an LLM and a clearly stated goal. While prompts are ‚Äúmostly‚Äù natural language, there is more to writing a good prompt than just telling the model what you want. Designing a good prompt requires a combination of:

Understanding of the LLM: Some LLMs might respond differently to the same prompt and might even have keywords (e.g. ‚Äúendofpromt‚Äù) that will be interpreted in a particular way
Domain knowledge: Writing a prompt to e.g. infer a medical diagnosis, requires medical knowlede.
Iterative approach with some way to measure quality: Coming up with the ideal prompt is usually a trial and error process. It is key to have a way to measure the output better than a simple ‚Äúit looks good‚Äù, particularly if the prompt is meant to be used at scale.
Prompt Engineering is prompt design plus a few other important processes:

Design of prompts at scale: This usually involves design of meta prompts (prompts that generate prompts) and prompt templates (parameterized prompts that can be instantiated at run-time)
Tool design and integration: Prompts can include results from external tools that need to be integrated.
Workflow, planning, and prompt management: An LLM application (e.g. chatbot) requires managing prompt libraries, planning, choosing prompts, tools‚Ä¶.
Approach to evaluate and QA prompts: This will include definition of metrics and process to evaluate both automatically as well as with humans in the loop.
Prompt optimization: Cost and latency depend on model choice and prompt (token length).
Many of the approaches herein can be considered approaches to ‚Äúautomatic prompt design‚Äù in that they describe ways to automate the design of prompts at scale. In the bonus section you will find some of the most interesting prompt engineering tools and frameworks that implement these techniques. However, it is important to note that none of these approaches will get you to the results of an experienced prompt engineer. An experienced prompt engineer will understand and be aware of all of the following techniques and apply some of the patterns wherever they apply rather than blindly following a particular approach for everything. This, for now, still requires judgement and experience. This is good news for you reading this. You can learn and understand these patterns, but you won‚Äôt be replaced by any of these libraries. Take these patterns as a starting tools to add to your toolkit, but also experiment and combine them to gain experience and judgement.

Advanced techniquesPermalink
Here are the techniques I will be covering:

Chain of thought (CoT)
Automatic Chain of thought (Auto CoT)
The format trick
Tools, Connectors, and Skills
Automatic multi-step reasoning and tool-use (ART)
Self-consistency
Tree of thought (ToT)
Reasoning without observation (ReWoo)
Retrieval Augmented Generation (RAG)
Forward-looking active retrieval augmented generation (FLARE)
Reflection
Dialog-Enabled Resolving Agents (DERA)
Expert Prompting
Chains
Agents
Reason and Act (React)
Rails
Automatic Prompt Engineering (APE)
Guidance and Constrained Prompting
Chain of Thought (CoT)Permalink
As mentioned, this technique was already discussed in the previous post. However, it is very noteworthy and it is at the core of many of the newer approaches, so I thought it was worthwhile to include here again.

Chain of thought was initially described in the ‚ÄúChain-of-Thought Prompting Elicits Reasoning in Large Language Models‚Äù paper by Google researchers. The simple idea here is that given that LLMs have been trained to predict tokens and not explicitly reason, you can get them closer to reasoning if you specify those required reasoning steps. Here is a simple example from the original paper:



Note that in this case the ‚Äúrequired reasoning steps‚Äù are given in the example in blue. This is the so-called ‚ÄúManual CoT‚Äù. There are two ways of doing chain of thought prompting (see below). In the basic one, called zero-shot CoT, you simply ask the LLM to ‚Äúthink step by step‚Äù. In the more complex version, called ‚Äúmanual CoT‚Äù you have to give the LLM examples of thinking step by step to illustrate how to reason. Manual prompting is more effective, but harder to scale and maintain.



Automatic Chain of Thought (Auto-CoT)Permalink
As mentioned above, manual CoT is more effective than zero-shot. However, the effectiveness of this example-based CoT depends on the choice of diverse examples, and constructing prompts with such examples of step by step reasoning by hand is hard and error prone. That is where automatic CoT, presented in the paper ‚ÄúAutomatic Chain of Thought Prompting in Large Language Models‚Äù, comes into play.

The approach is illustrated in the following diagram:



You can read more details in the paper, but if you prefer to jump right into the action, code for auto-cot is available here.

The ‚Äúformat trick‚ÄùPermalink
LLMs are really good at producing an output in a specific format. I don‚Äôt know if the ‚Äúformat trick‚Äù is a common term, but I did hear Riley Goodside use it in one of his presentations, so that is good enough for me. You can use the format trick for practically anything. Riley illustrated it by producing a LateX preprint for ArXiv.

 

However, if you specify code as the output format in your prompts you can do even more surprising things like generating a complete powerpoint presentation in Visual Basic.

Tools, Connectors, and SkillsPermalink
Tools are generally defined as functions that LLMs can use to interact with the external world.

For example, in the following code from Langchain, a ‚ÄúGoogle tool‚Äù is instantiated and used to search the web:



Tools are also known as ‚ÄúConnectors‚Äù in Semantic Kernel. For example, here is the Bing Connector. Note that Semantic Kernel has a related concept of ‚ÄúSkill‚Äù. A skill is simply a function that encapsulates a functionality called by an LLM (e.g. summarize a text) but does not necessarily require a Connector or a Tool to access the external world. Skills are also sometimes called ‚Äúaffordances‚Äù in other contexts.

In the paper ‚ÄúToolformer: Language Models Can Teach Themselves to Use Tools‚Äù, the authors go beyond simple tool usage by training an LLM to decide what tool to use when, and even what parameters the API needs. Tools include two different search engines, or a calculator. In the following examples, the LLM decides to call an external Q&A tool, a calculator, and a Wikipedia Search Engine.



More recently, researchers at Berkeley have trained a new LLM called Gorilla that beats GPT-4 at the use of APIs, a specific but quite general tool.

Automatic multi-step reasoning and tool-use (ART)Permalink
ART combines automatic chain of thought prompting and tool usage, so it can be seen as a combination of everything we have seen so far. The following figure from the paper illustrates the overall approach:



Given a task and an input, the system first retrieves ‚Äúsimilar tasks‚Äù from a task library. Those tasks are added as examples to the prompt. Note that tasks in the library are written using a specific format (or parsing expression grammar to be more precise). Given those task examples, the LLM will decide how to execute the current task including the need to call external tools.

At generation time, the ART system parses the output of the LLM until a tool is called, at which point the tool is called and integrated into the output. The human feedback step is optional and is used to improve the tool library itself.

Self-consistencyPermalink
Self consistency, introduced in the paper ‚ÄúSelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models‚Äù, is a method to use an LLM to fact-check itself. The idea is a simple ensemble-based approach where the LLM is asked to generate several responses to the same prompt. The consistency between those responses indicates how accurate the response may be.



The diagram above illustrates the approach in a QA scenario. In this case, the ‚Äúconsistency‚Äù is measured by the number of answers to passages that agree with the overall answer. However, the authors introduce two other measures of consistency (BERT-scores, and n-gram), and a fourth one that combines the three.

Tree of Thought (ToT)Permalink
Trees of Thought are an evolution of the CoT idea where an LLM can consider multiple alternative ‚Äúreasoning paths‚Äù (see diagram below)



ToT draws inspiration from the traditional AI work on planning to build a system in which the LLM can maintain several parallel ‚Äúthreads‚Äù that are evaluated for consistency during generation until one is determined to be the best one and is used as the output. This approach requires to define a strategy regarding the number of candidates as well as the number of steps/thoughts after which those candidates will be evaluated. For example, for a ‚Äúcreative writing‚Äù task, the authors use 2 steps and 5 candidates. But, for a ‚Äúcrossword puzzle‚Äù task, they keep up to a max of 10 steps and use BFS search.

Reasoning without observation (ReWOO)Permalink
ReWOO was recently presented in the paper ‚ÄúReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models‚Äù. This approach addresses the challenge that many of the other augmentation approaches in this guide present by increasing the number of calls and tokens to the LLM and therefore increasing cost and latency. ReWOO not only improves token efficiency but also demonstrate robustness to tool failure, and also shows good results in using smaller models.

The approach is illustrated in the diagram below. Given a question, the Planner a comprehensive list of plans or meta-plan prior to tool response. This meta-plan instructs Worker to use external tools and collect evidence. Finally, plans and evidence are sent to Solver who composes the final answer.



The following image, also from the paper, illustrates the main benefit of the approach by comparing to the ‚Äústandard‚Äù approach of reasoning with observations. In the latter, the LLM is queried for each call to a Tool (observation), which incurs in lots of potential redundancy (and therefore cost, and latency).



Retrieval Augmented Generation (RAG)Permalink
RAG is a technique that has been used for some time to augment LLMs. It was presented by Facebook as a way to improve BART in 2020 and released as a component in the Huggingface library.

The idea is simple: combine a retrieval component with a generative one such that both sources complement each other (see diagram below from the paper.



RAG has become an essential component of the prompt engineer‚Äôs toolkit, and has evolved into much more complex approaches. In fact, you can consider RAG at this point almost as a concrete case of Tools, where the tool is a simple retriever or query engine.

The FastRAG library from Intel includes not only the basic but also more sophisticated RAG approaches like the ones described in other sections in this post.

Forward-looking active retrieval augmented generation (FLARE)Permalink
FLARE is an advanced RAG approach where, instead of retrieving information just once and then generating, the system iteratively uses a prediction of the upcoming sentence as a query to retrieve relevant documents to regenerate the sentence if it the confidence is low. The diagram below from the paper clearly illustrates the approach.



Note that the authors measure confidence by setting a probability threshold for each token of the generated sentence. However, other confidence measures might be possible.

ReflectionPermalink
In the Self-consistency approach we saw how LLMs can be used to infer the confidence in a response. In that approach, confidence is measured as a by-product of how similar several responses to the same question are. Reflection goes a step further and tries to answer the question of whether (or how) we can ask an LLM directly about the confidence in its response. As Eric Jang puts it, there is ‚Äúsome preliminary evidence that GPT-4 possess some ability to edit own prior generations based on reasoning whether their output makes sense‚Äù.

The Reflexion paper proposes an approach defined as ‚Äúreinforcement via verbal reflection‚Äù with different components. The actor, an LLM itself, produces a trajectory (hypothesis). The evaluator produces a score on how good that hypothesis is. The self reflection component produces a summary that is stored in memory. The process is repeated iteratively until the Evaluator decides it has a ‚Äúgood enough‚Äù answer.



Dialog-Enabled Resolving Agents (DERA)Permalink
DERA, developed by my former team at Curai Health for their specific healthcare approach defines different agents that, in the context of a dialog take different roles. In the case of high stakes situations like a medical conversation, it pays off to define a set of ‚ÄúResearchers‚Äù and a ‚ÄúDecider‚Äù. The main difference here is that the Researchers operate in parallel vs. the Reflexion Actors that operate sequentially only if the Evaluator decides.



Expert PromptingPermalink
This recently presented prompting approach proposes to ask LLMs to respond as an expert. It involves 3 different steps:

Ask LLM to identify experts in a given field related to the prompt/question
Ask LLM to respond to the question as if it was each of the experts
Make final decision as a collaboration between the generated responses
Introduced in the (controversial) ‚ÄúExploring the MIT Mathematics and EECS Curriculum Using Large Language Models‚Äù beats other approaches like CoT or ToT. It can also be seen as an extension of the Reflection approach

ChainsPermalink
According to LangChain, which I am going to consider the authoritative source for anything Chains, a Chain is ‚Äújust an end-to-end wrapper around multiple individual components‚Äù

Now, of course, there are multiple types of Chains where you can combine different types and number of components in increasing complexity. In the simplest case, a chain only has a Prompt Template, a Model, and an Output Parser.

Very quickly though, Chains can become much more complex and involved. For example, the Map Reduce chain running an initial prompt on each chunk of data and then running a different prompt to combine all the initial outputs.

Since the process of constructing and maintaining chains can become quite an engineering task, there are a number of tools that have recently appeared to support it. The main one is the already mentioned LangChain. In ‚ÄúPromptChainer: Chaining Large Language Model Prompts through Visual Programming‚Äù, the authors not only describe the main challenges in designing chains, but also describe a visual tool to support those tasks. There is a tool with the exact same name and a similar approach available in Beta here. I am told this one has nothing to do with the authors of the original paper though. I haven‚Äôt used, so I can‚Äôt vouch for (or against) it.



AgentsPermalink
An agent is an LLM that has access to tools, knows how to use them, and can decide when to do so depending on the input (See here and here ).



Agents are not trivial to implement and maintain, that is why tools like Langchain have become a starting point for most people interested in building one. The ‚Äúpopular‚Äù Auto-GPT and also is just another toolkit to implement LLM agents.

Reason and act (React)Permalink
React is a specific approach to designing agents introduced by Google in ‚ÄúReAct: Synergizing Reasoning and Acting in Language Models‚Äù. This method prompts the LLM to generate both verbal reasoning traces and actions in an interleaved manner, which allows the model to perform dynamic reasoning. Importantly, the authors find that the React approach reduces hallucination from CoT. However, this increase in groundedness and trustworthiness, also comes at the cost of slightly reduced flexibility in reasoning steps (see the paper for more details).



As with chains and standard agents, designing and maintaining React agents is a pretty involved task, and it is worth using a tool that supports this task. Langchain is, again, the de facto standard for agent designs. Using Langchain, you can design different kinds of React agents such as the conversational agent, that adds conversational memory to the base (zero-shot) React agent (see here for examples and details).

RailsPermalink
A rail is simply a programmable way to control the output of an LLM. Rails are specified using Colang, a simple modeling language, and Canonical Forms, templates to standardize natural language sentences (see here )



Using rails, one can implement ways to have the LLM stick to a particular topic (Topical rail), minimize hallucination (Fact checking rail) or prevent jailbreaking (Jailbreaking rail).

Automatic Prompt Engineering (APE)Permalink
APE refers to the approach in which prompts are automatically generated by LLMs rather than by humans. The method, introduced in the ‚ÄúLarge Language Models Are Human-Level Prompt Engineers‚Äù paper, involves using the LLM in three ways: to generate proposed prompts, to score them, and to propose similar prompts to the ones scored highly (see diagram below).



Constrained PromptingPermalink
‚ÄúConstrained Prompting‚Äù is a term recently introduced by Andrej Karpathy to describe approaches and languages that allow us to interleave generation, prompting, and logical control in an LLM flow.

Guidance is the only example of such an approach that I know although one could argue that React is also a constrained prompting approach. The tool is not so much a prompting approach but rather a ‚Äúprompting language‚Äù. Using guidance templates, you can pretty much implement most if not all the approaches in this post. Guidance uses a syntax based on Handlebars that allows to interleave prompting and generation, as well as manage logical control flow and variables. Because Guidance programs are declared in the exact linear order that they will be executed, the LLM can, at any point, be used to generate text or make logical decisions.



Prompt Engineering tools and frameworksPermalink
As we have seen throughout this guide, it is hard to implement

Langchain
Langchain is the most popular prompt engineering toolkit. While it initially mostly focused on supporting Chains, it now supports Agents and many different Tools for anything from handling memory to browsing.
Semantic Kernel
This toolkit developed by Microsoft in C# and Python is designed around the idea of skills and planning. That being said, at this point it also supports chaining, indexing and memory access and plugin development.
Guidance
Guidance is a more recent prompt engineering library also from Microsoft. Based on a templating language (see above) it supports many of the techniques in this post.
Prompt Chainer
Visual tool for prompt engineering
Auto-GPT
Popular tool for designing LLM agents
Nemo Guardrails
Recent tool by NVidia to build rails (see above) to make sure your LLM behaves as it should
LlamaIndex
Toolkit for managing the data that goes into an LLM application with mostly data connectors/tools.
FastRAG
From Intel, includes not only the basic RAG approach but also more sophisticated RAG approaches like the ones described in other sections in this post.

Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling Weijia Xu1 Andrzej Banburski-Fahey1 Nebojsa Jojic1 Abstract arXiv:2305.09993v2  [cs.LG]  23 May 2024 Weintroduce Reprompting, an iterative sampling algorithm that automatically learns the Chain-ofThought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, Reprompting infers the CoT recipes that work consistently well for a set of training samples by iteratively sampling new recipes using previously sampled recipes as parent prompts to solve other training problems. We conduct extensive experiments on 20 challenging reasoning tasks. Results show that Reprompting outperforms humanwritten CoT prompts substantially by +9.4 points on average. It also achieves consistently better performance than the state-of-the-art prompt optimization and decoding algorithms. 1. Introduction Few-shot prompting with large language models (LLMs) has revolutionized the landscape of natural language processing. Given natural language instructions and a few demonstrations as in-context examples, LLMs can quickly adapt to new tasks, approaching or even surpassing the performance of models fine-tuned on larger datasets on a wide range of tasks (Brown et al., 2020). However, such prompting techniques fall short on tasks that require multi-step reasoning and constraint propagation (Wei et al., 2022), such as logical deduction in the Big-Bench Hard benchmark (Suzgun et al., 2022). To address these limitations, prior works proposed to teach LLMs to reason step by step like humans by prompting them with chain-of-thought (CoT) reasoning steps for a few example problems (Wei et al., 2022). Despite the improved performance, such a method requires human experts with not only the task knowledge but also an understanding of how prompting works to craft the CoT prompt for each task (Zamfirescu-Pereira et al., 2023), which limits 1Microsoft Research, Redmond, USA. Correspondence to: Weijia Xu <weijiaxu@microsoft.com>. Proceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). the scalability and generalizability of the method. Furthermore, a problem can be reasoned in many different ways, and some of them may work well on some LLMs but not on others. To fairly compare the performance of various LLMs on each task, we need to findtheCoTpromptthatworksbest for each model in a feasible way, which remains a challenge. In this paper, we propose Reprompting, an iterative sampling algorithm that automatically finds effective CoT prompt for each model given a few question-answer pairs without human intervention. Specifically, the algorithm aims to infer a set of CoT recipes that perform consistently well as in-context examples for a set of training problems. We frame it as a problem of sampling from a joint distribution of CoT recipes given the training question-answer pairs, which is infeasible to characterize directly but can be approached using Gibbs sampling‚Äì we initially sample a set of recipes through zero-shot prompting, expand the set with newrecipes sampled iteratively by using previously sampled recipes as parent prompts to solve a different training problem, and weed out the least-fit recipes that lead to wrong answers. Thus, the algorithm will eventually converge to a set of recipes that share similar chains of thought for effectively solving the training problems. These CoT recipes optimized on the training set then serve as effective CoT prompts for solving unseen test problems. We evaluate Reprompting on 20 tasks from three reasoning benchmarks including Big-Bench Hard (BBH) (Suzgun et al., 2022), GSM8K (Cobbe et al., 2021) and MATH(Hendrycks et al., 2021) using ChatGPT (OpenAI, 2023) and InstructGPT (Ouyang et al., 2022) as LLMs. Compared with human-written CoT prompts, Reprompting achieves +9.4 higher accuracy on average. It also consistently outperforms self-consistency decoding (Wang et al., 2022b), Auto-CoT (Zhang et al., 2022) and Automatic Prompt Optimization (Pryzant et al., 2023) by 11‚Äì33 points on average. Furthermore, Reprompting facilitates model combination by using different LLMs for initializing and sampling new recipes. Empirically, leveraging ChatGPT to sample initial recipes for InstructGPT brings up to +71 point improvements over using InstructGPT alone and even outperforms ChatGPT alone on certain tasks. Lastly, our results confirm that the CoT recipes that work well on one model 1Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling Figure 1: An example that ChatGPT can propose various different solutions to the same problem in zero-shot. may work poorly on another, even when the latter may approach the best performance using prompts optimized for itself. These findings emphasize the need to optimize the prompt for each model for fair comparisons. 2. Reprompting: Prompt Inference Through Gibbs Sampling 2.1. In-Context Learning In-context learning has become the cornerstone of evaluating large language models (LLMs) (Brown et al., 2020; Srivastava et al., 2022). To facilitate this evaluation approach, data is provided for a large number of different tasks, with each task consisting of dozens or, more often, hundreds of instances with varying problem setup and question texts xi and their corresponding text answers yi, where i ‚àà[1..N] and N is the number of problem instances for the task. Formally, in-context learning infers the answer for a given test question x by prompting an LLM with a set of demonstration examples {xi,yi}K i=1: ÀÜ y ‚àº pLLM(y|{xi,yi}K i=1,x) (1) The performance of in-context learning can be significantly enhanced by incorporating auxiliary knowledge or humanwritten instructions in a prompt (Shwartz et al., 2020; Zelikman et al., 2022; Nye et al., 2021), particularly in the form of Chain-of-Thought (CoT) reasoning (Wei et al., 2022; Wang et al., 2022b; Zhou et al., 2022; Creswell et al., 2022; Wang et al., 2022a; Liu et al., 2022; Kojima et al., 2022; Li et al., 2022). In-context learning with CoT (Wei et al., 2022) can be seen in a similar light, statistically. In addition to the questionanswer pairs {xi,yi}, the CoT prompt also contains worked out step-by-step reasoning ‚Äúrecipes‚Äù zi in text, which are inserted between the question and answer: {xi,zi,yi}. These recipes can play two roles. First, they further explain the intent of the question xi, as a small collection of questionanswer pairs alone may be insufficient to disambiguate among different patterns an LLM might detect. The second role is more important: it provides step-by-step guidance on one problem and thus teaches an LLM to solve similar problems following the same routine as it continues the text conditioned on the previous tokens. In the extreme, with prompts that strictly regiment self-attention, GPT models can be turned into Turing Machines to execute standard computer algorithms (Jojic et al., 2023). In practice, the CoT prompts commonly used in prior work fall somewhere between colloquial explanations and regimented recipes. Formally, in-context learning with CoT infers the answer for a given test question x by prompting an LLM with an optional instruction message m and a set of demonstration examples with step-by-step solutions {xi,zi,yi}K i=1: ÀÜ z, ÀÜ y ‚àº pLLM(z,y|{xi,zi,yi}K i=1,x,m) (2) Here, m is a textual message that instructs the model to generate the step-by-step solution zj before the answer text yj and the specific format to present the answer.1 It can be task-specific or generic, as in the case of our experiments. Such an instruction message can trigger instruction-tuned LLMs to generate step-by-step solutions given [xj,m] alone without any demonstration examples (i.e. K = 0), as illustrated in Figure 1. These solutions follow varying styles and often lead to incorrect answers. However, we argue that good recipes for solving the set of problems on a given task can evolve from these zero-shot solutions. In the next section, we introduce Reprompting, an iterative sampling algorithm that automatically produces the CoT recipes for a given set of problems without human intervention. 2.2. Prompt Inference Through Gibbs Sampling We introduce the Reprompting algorithm, which aims to f ind a set of CoT recipes zi that work consistently well as few-shot in-context examples for a dataset {xi,yi}N i=1. Specifically, we formulate it as the problem of sampling from a joint distribution p(z1,z2,...zN|{xi,yi}N i=1,m) (3) such that z1...N are generalized enough so that given any test question x, the distribution over z and y is approximately invariant to the choice of the K-shot CoT recipes: pLLM(z,y|{xi,zi,yi}N i=1,x,m) ‚âàpLLM(z,y|{xi,zi,yi}i‚ààS,x,m), ‚àÄS ‚äÇ [1,N],|S| = K (4) Without characterizing the joint distribution, we can use Gibbs sampling (Geman & Geman, 1984) to generate such 1This enables us to separate the generated answer yj from the step-by-step solution zj and forces the model to stop after generating the answer. 2Reprompting:AutomatedChain-of-ThoughtPromptInferenceThroughGibbsSampling samples{z1,z2,...zN}byfirstsampling{z1,z2,...zN}independentlyfromthedistributionsp(zj|xj,yj), andtheniterativelydrawingsamples fromtheconditionaldistributionsp(zj|z1,...,zj‚àí1,zj+1,...zN,{xi,yi}N i=1,m). Basedon theproperty(4)ofthejointdistribution,wehavethefollowingapproximation: p(zj|z1,...,zj‚àí1,zj+1,...zN,{xi,yi}N i=1,m) =pLLM(zj|{xi,zi,yi}i=j,xj,yj,m) ‚àùpLLM(zj,yj|{xi,zi,yi}i=j,xj,m) ‚âàpLLM(zj,yj|{xi,zi,yi}i‚ààSj ,xj,m), ‚àÄSj‚äÇ[1,N]\{j},|Sj|=K (5) Thus, we can sample zj by randomlypickingKdata points (excluding j)andthensamplingzjwithweights proportionaltotheconditionalprobability pLLM(zj,yj|{xi,zi,yi}i‚ààSj ,xj,m) =pLLM(zj|{xi,zi,yi}i‚ààSj ,xj,m) ¬∑pLLM(yj|{xi,zi,yi}i‚ààSj ,xj,m,zj) (6) Onewaytoapproximateit is tosampleseveral ÀÜ zj from theLLMconditionedon{xi,zi,yi}i‚ààSj ,xjandm,compute theweight for each ÀÜ zj using themodel‚Äôsprobabilityof thecorrectansweryj conditionedon{xi,zi,yi}i‚ààSj ,xj,m andÀÜ zj, andsampleazj from{ÀÜ zj}basedontheweights. Inpractice,however, themodellikelihoodofagiventext maybeinaccessible. Thus,weapproximateitusingrejectionsampling‚Äìwesamplezj bysampling ÀÜ zj and ÀÜ yj frompLLM(z,y|{xi,zi,yi}i‚ààSj ,xj,m)andthenreject ÀÜ zjwith aprobabilityofprej if ÀÜ yj=yj. Otherwise,weaccept ÀÜ zj andupdatethesample.Algorithm1showsthecomplete Repromptingalgorithmconsistingoftheinitializationand iterativesamplingsteps.Notethatwesettherejectionprobabilityprej inawaythatallowssolutionsthatleadtoincorrectanswerstobekeptoccasionally,asthesesolutionsmay stillcontainusefulsegmentsthatevolveintogoodrecipes throughReprompting. Based on the properties of Gibbs sampling (Casella &George, 1992; Roberts&Smith, 1994), the algorithmshouldconvergetothepointwheretheprobability pLLM(zj,yj|{xi,zi,yi}i‚ààSj ,xj,m)ishighandagnostictothe choiceofSj,whichleadstoasetof{zj}thatworkwellas apromptforsolvingsimilarproblemsinaseparatetestset. Thealgorithmcanalsobeviewedasavariantofevolutionaryalgorithms:1)First,wegeneratetheinitialpopulation ofindividuals(whereeachindividualisaCoTrecipegiven aproblem).2)Next,werepeatthefollowingregeneration stepsiteratively:2a)wefirstevaluatethefitnessofeachCoT recipebycomparingtheanswerthatfollowstherecipewith thecorrectanswerandweedouttheleast-fitrecipes;2b)we thenbreednewindividualsthroughcrossoverandmutation Algorithm1:Repromptingalgorithm Input:Trainingset{xi,yi}N i=1,numberofexamplesin thepromptK,numberofiterationsM,rejection probabilityprej,theinitializationmodelLLM1 andthesamplingmodelLLM2 1 Initialization: 2 foreach jdo 3 zj‚Üê/ 0 4 SampleÀÜ zj,ÀÜ yj‚àºpLLM1 (z,y|xj,m) 5 Sampleu‚àºUniform([0,1]) 6 if ÀÜ yj=yjoru>prej then 7 zj‚ÜêÀÜ zj 8 end 9 end 10 Sampling: 11 repeat 12 Randomlyselect j‚àà[1,N] 13 RandomlyselectSj‚äÇ[1,N]\{j}ofsizeK 14 SampleÀÜ zj,ÀÜ yj‚àºpLLM2 (z,y|{xi,zi,yi}i‚ààSj ,xj,m) 15 Sampleu‚àºUniform([0,1]) 16 if ÀÜ yj=yjoru>prej then 17 zj‚ÜêÀÜ zj 18 end 19 untilconvergenceorMiterationsarereached byrandomlyselectingKrecipesfromthepopulationas parentrecipes,whicharethenusedtoprompttheLLMto generaterecipesforanewproblem.Byrepeatingthe2aand 2bsteps,initialrecipescanberecombined(Figure4)and evolveintobetterrecipes(Figure3)throughiterations.And eventually,thefittestrecipes(i.e.onesthatcanbefollowed tosolvesimilarproblems)willsurvive. Duringtesting,weselectKtuples{xi,zi,yi}fromtheinferred{zj}basedonthetrainingaccuracywhenusingeach tupleindividuallyinaprompt. 3.ExperimentalSetup We evaluate the Reprompting algorithm against various baselines including zero-shot, few-shot, Chainof-Thought (CoT), Chain-of-Thought combined with self-consistency decoding (Wang et al., 2022b), AutoCoT(Zhanget al., 2022) andAutomaticPromptOptimization (Pryzant et al., 2023) on20 challenging reasoningtasks, including12challengingtasks intheBigBenchHard (BBH) benchmark (Suzgunet al., 2022),2 2TheBBHtasksincludeLogicalDeduction,GeometricShapes, ObjectCounting,PenguinsinaTable,TemporalSequences,Date Understanding,FormalFallacies,MovieRecommendation,ReasoningAboutColoredObjects,RuinNames,SalientTranslation ErrorDetection,andWordSorting. 3Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling GSM8K(Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). We choose both tasks that have been shown to benefit substantially from human-written CoT recipes, such as Logical Deduction, Geometric Shapes, Temporal Sequences, GSM8KandMATH,andtasks on which CoT does not improve much or does not improve consistently over zero-shot prompting, such as Formal Fallacies, Movie Recommendation and Word Sorting. 3.1. Reprompting Setup For each task, we randomly select 20 training examples from the Big-Bench dataset excluding the test examples in the BBH benchmark.3 We experiment with having k ‚àà {1,3} clones of the same training example in the set {xi,yi}N i=1 to allow for more diverse recipe samples (so the number of recipes we need to sample from the joint distribution (3) is N = 20‚àók) and choose k that obtains the highest training accuracy. We set the number of examples in the prompt by K =5. We run Reprompting for a maximum of M =20,000 iterations. We allow for early stopping if the average training accuracy stops increasing for 1,000 iterations. For the rejection probability, we experiment with prej ‚àà {0.95,0.99} and choose prej = 0.99 as it leads to higher training accuracy on various tasks. 3.2. Baselines Prompting Baselines For zero-shot prompting, we only include the test question xi and the special message m in the prompt, which triggers the model to generate a step-by-step solution prior to the answer text. For few-shot prompting, werandomly select 20 training examples in the same way as in Reprompting and concatenate these examples in the form of question-answer pairs in the prompt, followed by the test question. For CoT prompting, we use the human-written CoT prompts from Suzgun et al. (2022). For CoT with selfconsistency decoding, we use the same CoT prompts and follow Wang et al. (2022b) by sampling 10 reasoning paths per question and taking the majority vote on the answer. For both approaches, we randomly select 20 training examples in the same way as in Reprompting.4 3Except for Penguins in a Table where there are only three samples in the Big-Bench dataset that are excluded from BBH, so we randomly select 17 more examples from BBH into the training set. 4Recent prompting methods that are more annotation-intensive, such as Complex-CoT (Fu et al., 2022) and Progressive-Hint Prompting (Zheng et al., 2023), are shown to outperform Reprompting by 3.3‚Äì5.6 points on GSM8K. However, these methods leverage substantially more human-annotated examples (e.g. 7.5K annotated examples on GSM8K) than Reprompting, thus they are not directly comparable. Prompt Optimization Baselines We also compare Reprompting with two previous state-of-the-art prompt optimization algorithms, including Auto-CoT (Zhang et al., 2022) and APO (Pryzant et al., 2023). For Auto-CoT, since the original Auto-CoT algorithm differs from our setting as it focuses on the unsupervised setting without exploiting any labeled examples, we adapt the algorithm to our few-shot setting where it follows the original algorithm to generate diverse CoT recipes through zero-shot prompting but selects the demonstration examples based on the training accuracy whenusedindividually in a prompt.5 We also evaluate APO, a recently proposed nonparametric prompt optimization algorithm that uses LLMs to generate ‚Äútextual gradient‚Äùcriticism of the current prompt‚Äì based on training samples and edit the prompt accordingly. The algorithm has been shown to outperform other prompt optimization methods, such as TEMPERA (Zhang et al., 2023), Automatic Prompt Engineering (Zhou et al., 2023), and AutoGPT.6 3.3. Large Language Models (LLMs) We experiment with two powerful LLMs including ChatGPT (gpt-3.5-turbo; OpenAI (2023)) and InstructGPT (textdavinci-003; Ouyang et al. (2022)). We also experiment with a combo model for Reprompting where we use ChatGPT as LLM1 for initialization and InstructGPT as LLM2 for sampling. For both LLMs, we set the maximum number of output tokens to 500, top_p = 0.5, zero frequency and presence penalty. Additionally, we include ‚ÄúEND‚Äù as the stop word. We set the temperature to 1.0 for Reprompting and 0.0 for testing. 3.4. Evaluation Protocol Weextract the final answer from the model output by extracting the text between ‚Äú<answer>‚Äù and ‚Äú</answer>‚Äù, except for the CoT baseline where we extract the final answer in the same way as in Suzgun et al. (2022). We measure accuracy based on exact match by comparing the extracted answer with the ground truth. 4. Results 4.1. Main Results Wefirst compare the performance of Reprompting with all the baselines on five BBH tasks. As shown in Table 1, results confirm the previous finding that few-shot in-context prompting improves the performance over zero-shot (Brown et al., 2020) and that CoT prompting outperforms both zeroshot and few-shot prompting by a large margin. However, human-written CoT prompting requires costly prompt en5The original Auto-CoT algorithm selects the demonstration examples based on the diversity of the demonstration questions. 6https://news.agpt.co/ 4Reprompting:AutomatedChain-of-ThoughtPromptInferenceThroughGibbsSampling BBHTask SOTA ZS FS CoT CoT+SC APO AutoCoT Reprompting ChatGPT ChatGPT InsGPT Chat+Ins Logical 60.4 35.1 46.4 63.1 62.7 28.0 53.2 66.3 53.7 60.0 Geometric 56.0 13.6 20.0 58.0 60.0 52.0 52.4 72.8 40.8 64.4 ObjectCount 93.2 52.4 46.8 95.6 95.2 74.8 88.8 97.2 42.8 99.6 Penguins 81.5 50.7 60.3 67.1 71.2 45.2 85.6 85.6 78.1 82.9 Temporal 96.8 38.4 41.2 66.8 66.8 50.4 80.8 93.2 28.4 99.2 Average 77.6 38.0 42.9 70.1 71.2 50.1 72.2 83.0 48.8 81.2 Table1:Performanceofseverallargelanguagemodels(LLMs)usingRepromptingversusthebaselinepromptingandpromptoptimization methodsonBig-BenchHard(BBH)tasks.SOTAreferstothestate-of-the-artperformanceamongInstructGPT(text-davinci-002;Ouyang etal.(2022)),Codex(Chenetal.,2021),andPaLM540B(Chowdheryetal.,2022)usingCoTpromptingfromSuzgunetal.(2022). WealsocompareRepromptingwithChatGPTusingZS(zero-shot),FS(few-shot),CoT,CoT+SC(CoTpromptingcombinedwith self-consistencydecoding(Wangetal.,2022b)),APO(automaticpromptoptimizationusingtextualgradient(Pryzantetal.,2023)), andAutoCoT(thefew-shotversionofAuto-CoT(Zhangetal.,2022)).ForReprompting,weshowtheperformanceofvariousLLMs‚ÄìincludingChatGPT(gpt-3.5-turbo;OpenAI(2023)),InstructGPT(text-davinci-003),andChat+Instruct(acomboversionthatuses ChatGPTforinitializationandInstructGPTatsamplingsteps). gineering, asnot allCoTrecipesworkequallywell on LLMs(Madaan&Yazdanbakhsh,2022;Jojicetal.,2023). Crucially,weshowthat usingReprompting, LLMscan achievebetterperformancecomparedtotheexistingCoT prompts,butwithoutrequiringanyhumanguidanceonhow tosolveproblemsstepbystep.Specifically,comparingthe performanceofChatGPTusingRepromptingversusthebest human-writtenCoTpromptsfromSuzgunetal.(2022),Repromptingachievesconsistentlyhigherscoresonalltasks. Next,wecompareRepromptingwithself-consistency(SC) decoding(Wangetal.,2022b). CoT+SCimprovesover CoTontwoof thefivetasks, but theimprovementsare not consistent. By contrast, Reprompting consistently outperformsCoT+SCby2‚Äì26pointsonallfivetasks. Additionally, we compare Repromptingwith existing promptoptimizationalgorithms.APOimprovesoverzeroshotpromptingonthreeoutoffivetasksbutunderperforms itonthetwotaskswherethemodelneedstosearchthrough awiderangeofstrategiestofindeffectivesolutions.Bycontrast,Repromptingconsistentlyoutperformszero-shotand CoTprompting,andimprovesoverAPOby20‚Äì43points onallfivetasks.WhencomparedagainstAuto-CoT(Zhang etal.,2022),Repromptingalsoarchiveshigheraccuracy by+11pointsonaverage. Insummary,Repromptingoutperformsstrongdecodingandpromptoptimizationbaselines by11‚Äì33pointsonaverage. ComparingtheperformanceofRepromptingondifferent LLMs,weobservethatInstructGPTunderperformsChatGPTonmosttasks.However,weshowthatbyusingChatGPTjustastheinitializationmodelLLM1tobootstrapInstructGPTasLLM2inReprompting,wecanimproveperformanceoverInstructGPTaloneby5‚Äì71pointsandachieve competitiveorevenbetterperformancethanChatGPTalone ontwoofthefivetasks.WeshowintheAppendixwhythat is:whileInstructGPTcanfollowagivenrecipeandevenbe usedforrecombiningandevolvingthem,itislesscapable ofgeneratingdiverseinitialsolutionsinazero-shotmanner. However, throughReprompting, we canuseChatGPT to‚Äúteach‚ÄùInstructGPTdiversestrategiesforsolvingthe trainingproblems,whicharethenrecombinedandevolved byInstructGPTintobetterCoTpromptsforitself. Furthermore,Table2showstheperformanceofRepromptingagainstzero-shot,few-shotandCoTprompting(allusingChatGPT)ontheremaining15tasks.7Repromptingstill outperformszero-shotandfew-shotpromptingconsistently andsubstantiallyby14-15pointsonaverage. Compared withCoT,Repromptingachievesbetterperformanceon 11outof15tasks.Onaverage,Repromptingoutperforms CoTby+8.2points. Interestingly, ontaskswhereCoT evenunderperformszero-shotprompting, suchasMovie Recommendation,SalientTranslationErrorDetection,and WordSorting,Repromptingstill improvesoverzero-shot promptingbylargemargins.ThissuggeststhatnotallCoT recipesimprovemodelperformance,andsomemayeven leadtodegradation.Thisfurtheremphasizestheneedfor algorithmslikeRepromptingfordiscoveringandoptimizing theCoTprompttobestexploitandcompareLLMs. Overall,thesefindingshighlightthepotentialofRepromptingasapowerfulmethodforautomatingCoTpromptingon awiderangeoftasks. 4.2.QuantitativeAnalysis AblationStudy Weconductanablationstudyontherejectionsamplingandrecombinationprocess.ResultsinTable3 showthat,withoutrejectionsampling,thetestperformance degradessubstantiallyby25pointonaverage. Always 7BasedonthemainresultsinTable1,CoT+SCandAuto-CoT aremorecomplicatedthanCoTbutonlyslightlyimprovesover CoT.Thus,weselectCoTasabaselinehere. 5Reprompting:AutomatedChain-of-ThoughtPromptInferenceThroughGibbsSampling 0 2000 4000 6000 8000 10000 Iterations 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 Average Training Accuracy (a)InstructGPT 0 2000 4000 6000 8000 10000 Iterations 0.5 0.6 0.7 0.8 0.9 1.0 Average Training Accuracy (b)ChatGPT 0 4000 8000 12000 16000 20000 Iterations 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Average Training Accuracy (c)ChatGPT+InstructGPT Figure2:LearningcurvesoftheRepromptingalgorithmusingInstructGPT,ChatGPT,andthecomboChatGPT+InstructGPTmodelson theLogicalDeductiontask.They-axisshowstheaccuracyontrainingsamplesaveragedoverthecurrentandallpreviousiterations. ZS FS CoT Reprompting BBH Date 63.6 46.4 76.8 76.4 Formal 49.2 53.6 48.4 56.8 Movie 59.2 72.4 25.6 78.4 ColoredObj 66.8 48.8 76.0 74.0 Ruin 53.2 66.8 60.8 74.8 Salient 43.2 53.2 32.8 54.8 WordSort 58.0 72.0 46.0 73.2 GSM8K 45.6 26.5 75.6 79.5 MATH Algebra 37.6 23.7 52.0 53.1 Counting 17.1 19.8 26.6 32.3 Geometry 12.4 16.2 28.5 29.2 IntAlgebra 9.4 12.1 18.0 16.8 Number 20.8 17.1 32.9 33.3 Prealgebra 31.4 33.2 54.0 43.8 Precalculus 7.4 18.4 19.0 19.3 Average 38.3 38.7 44.9 53.0 Table2: PerformanceofChatGPTusingRepromptingversus ZS(zero-shot),FS(few-shot),andCoTpromptingmethodson sevenadditionaltasksfromBig-BenchHard(BBH)(Suzgunetal., 2022),GSM8K(Cobbeetal.,2021)andMATH(Hendrycksetal., 2021). rejectingsolutionsthatleadtoincorrectanswersalsocauses adegradationof8point.Additionally,notallowingmultiple solutionstoberecombinedwhensamplingnewsolutions attheiterativesamplingstagealsohurtsperformance. DothegeneratedCoTrecipesgeneralizeacrossmodels? Wetest thebest-performingCoTrecipesoptimizedwith InstructGPT,ChatGPT,orInstructGPT+ChatGPTthrough RepromptingonbothInstructGPTandChatGPT.Asshown inTable4,theCoTrecipesoptimizedforonemodelmaynot prej=0 prej=1 NoRec Orig. Logical 56.3 61.9 54.7 66.3 ObjectCount 52.0 97.2 95.6 97.2 Temporal 74.8 74.4 90.4 93.2 Average 61.0 77.8 80.2 85.6 Table3:Ablationstudyonrejectionsampling(includingnorejection(prej=0)andalwaysrejecting(prej=1))andrecombination(NoRecrepresentsRepromptingwithoutrecombinationofpreviouslysampledrecipes)onLogicalDeduction,ObjectCounting, andTemporalSequencesfromBig-BenchHard(BBH)(Suzgun etal.,2022).TheOrig.columnrepresentsthestandardRepromptingalgorithmwithoutablation. Tasks InsGPT ChatGPT Logical 65.9 66.3‚àó Geometric 53.6 72.8‚àó ObjectCount 99.6‚àó 96.8 Penguins 82.2 85.6‚àó Temporal 99.2‚àó 81.6 Table4:TestingthebestperformingCoTpromptlearnedonChatGPT,InstructGPTorInstructGPT+ChatGPTthroughReprompting onbothChatGPTandInstructGPT.Thesuperscript‚àódenotesthe modelusedasLLM2inReprompting. workaswellforothermodels.Specifically,weobservethat ontaskssuchasLogicalDeductionandObjectCounting, thebestCoTrecipesachievesimilarperformanceonboth InstructGPTandChatGPT.However,onGeometricShapes andTemporalSequences,thebestCoTpromptsoptimized forLLM2workwellonLLM2,butpoorlywiththeother LLM‚ÄìusingthemontheotherLLMleadsto18‚Äì19points loweraccuracythantestingwithLLM2 (seeexamplesin FigureA.2). Onsuchtasks,usingthepromptoptimized for thetestingLLMimprovesaccuracyby11‚Äì12points overthesametestingLLMwithpromptoptimizedforother LLMs.Theseresultssuggestthat,tomakeafaircomparison 6Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling Figure 3: An example of how the CoT recipes evolve through Reprompting. In the left-most recipe, the model (ChatGPT) first reorders the constraints so that the ones with absolute ranking positions are considered prior to the ones with relative positions (highlighted in dark blue). Next, the model attempts to deduce the objects at specific positions but makes a mistake (see the red underlined part). Despite the error, this recipe still provides a useful strategy for solving similar problems‚Äì when it is used in a prompt to solve another problem, the model first adopts the same strategy to reorder the constraints and then proposes another way to deal with the constraints (highlighted in orange). Although the resulting solution still contains errors, it makes a good recipe for solving this type of problem. Thus, when using it in a new prompt to solve yet another problem, the model can follow the same recipe and deduce the correct answer. between different LLMs, one needs to optimize the CoT prompt for each model. Reprompting improves CoT recipes over iterations. In Figure 2, we plot the average training accuracy (averaged over iterations up to the current iteration) over training iterations on Logical Deduction. For all three model variants, the initial training accuracy is relatively low, but it gradually increases (with occasional fluctuations) over iterations until convergence. This is the result of evolution and recombination of the recipes associated with training examples. Compute andResources WeusetheOpenAIAPIsfor all our experiments.8 Running Reprompting costs around $80 (in US dollars) on gpt-3.5-turbo and $800 on text-davinci003 based on the standard pricing,9 while being exempted from any human cost. By contrast, CoT prompting requires manual prompt construction and engineering, which costs not only human labor (including the cost for humans to get familiar with the task itself and how LLM prompting works, write down various CoT solutions for each problem, test and optimize the solutions on the LLM) but also LLM queries, but these costs are typically neglected in previous works. In addition, previous works typically compare different LLMs using the same CoT prompt. While this strategy avoids additional costs for custimizing CoT prompt for each 8https://platform.openai.com/docs/ api-reference?lang=python 9https://openai.com/pricing LLM(even with Reprompting, one can also save the cost by running it with ChatGPT and using the inferred CoT prompt on other LLMs), it risks making unfair comparisons as we have shown in Table 4 that the CoT prompt that works well on one model may be sub-optimal for another. 4.3. Qualitative Analysis Weobservethat even modeloutputscontaining errors and unreasonable deductions can evolve into a high-quality recipe through Reprompting. This is illustrated by the Logical Deduction example in Figure 3, when K = 1, where the model initially generates a recipe that is erroneous and contains illogical deductions. However, when this recipe is used as the new prompt for solving a similar problem, the model is able to exploit parts of the recipe and propose an alternative way to continue reasoning. Although the subsequent recipe still contains errors, it aids the model in correctly solving other problems when incorporated into a prompt. As a result, such recipes will be populated on other training samples, while the recipes that lead to low accuracy will eventually die out. Reprompting combines fragments from different recipes into a better one. Reprompting benefits from having multiple examples in the prompt, which allows the model to integrate various segments from different prompt recipes into a new recipe. As illustrated by the Object Counting examples in Figure 4, the model can combine large segments of reasoning steps, as well as small segments that address 7Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling Figure 4: Examples of how fragments from different recipes in a prompt can be (re)combined into a better recipe to solve a new problem through Reprompting. distinct cases to solve a more complex problem. The resulting prompts sometimes, but not always, share similarities with the human-written prompts (See the Appendix). 5. Related Work In-Context Learning is an emergent ability of LLMs as they scale up in model sizes and training data, where an LLMscan learn to perform a task from a few examples in the context (which is also referred to as few-shot prompting) (Brown et al., 2020). It has been shown to achieve promising few-shot and even zero-shot performance on various natural language processing (Brown et al., 2020; Schick &Sch√ºtze, 2020; Perez et al., 2021) and program synthesis (Austin et al., 2021) tasks. Reasoning via Chain-of-Thought Prompting Chain-ofThought (CoT) prompting is a technique that enables LLMs to perform complex reasoning tasks by prompting them with a few examples with step-by-step solutions (Wei et al., 2022; Suzgun et al., 2022). CoT prompting has been shown to improve performance on various reasoning tasks, such as arithmetic reasoning (Wei et al., 2022; Zhou et al., 2022), symbolic reasoning (Wei et al., 2022; Zhou et al., 2022), multi-hop question answering (Press et al., 2022; Arora et al., 2022), and natural language inference (Wang et al., 2022b). However, designing effective CoT prompts requires human experts with an understanding of both the task and the prompting technique (Zamfirescu-Pereira et al., 2023), which limits the scalability and generalizability of CoT prompting. Several works have attempted to automate the process of CoTpromptdiscovery. Zhang et al. (2022) proposed AutoCoT, whichusesLLMstogenerateCoTsolutionsfordiverse training questions in zero-shot and integrates the generated CoT solutions in the prompt for solving test questions. This method differs from Reprompting in that: 1) it focuses on the unsupervised setting and exploits a large set of example questions without annotated answers, and 2) it relies more heavily on the correctness of the zero-shot recipes as it does not have any iterative algorithm (as in Reprompting) to further improve the recipes. In our experiments, we adapted Auto-CoT to the few-shot setting and showed that Reprompting outperforms the few-shot version of Auto-CoT. Deng et al. (2022); Zhang et al. (2023) proposed to train an additional policy model to find the best prompt through reinforcement learning, but their approaches are limited to prompt optimization within a relatively small search space (i.e. it is restricted to the prompts that are either extremely short or within a small edit distance from an initial prompt). Zhou et al. (2023) proposed a method for automatically generating, scoring and selecting effective instruction messages mfor zero-shot chain-of-thought reasoning, which is orthogonal and can be potentially combined with our algorithm. Paranjape et al. (2023) introduced a framework that automatically retrieves demonstrations of related tasks from a task library and generates CoT solutions for the new task. However, this framework still requires collective human efforts to write demonstrations for a diverse set of tasks in the task library. In contrast, our Reprompting algorithm enables LLMs to solve complex reasoning tasks without any human guidance. Additionally, Yoran et al. (2023) proposed a multi-chain reasoning (MCR) method that prompts LLMs to combine pieces of information from multiple chains of thought to predict the final answer, which differs from our method in two ways: first, MCR combines multiple CoT solutions to the same question at test time, while Reprompting combines CoT solutions generated for different training questions before testing; second, MCR combines solutions only once, whereas Reprompting iteratively samples new 8Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling solutions and recombines them. As a result, Reprompting generates effective CoT recipes from only a few training examples, resulting in improved test performance without slowing down test inference. 6. Conclusion Weintroduce Reprompting, an automated prompt inference algorithm which, without human effort, discovers effective chain-of-thought (CoT) prompts for each task given a few question-answer pairs. Experiments on 20 challenging reasoning tasks show that Reprompting achieves +9.4 higher accuracy than human-written CoT on average. It also outperforms self-consistency decoding and the state-of-the-art prompt optimization algorithms by 11‚Äì33 points on average. Our results also suggest that LLM comparisons can be highly sensitive to the choice of CoT prompts, further emphasizing the need for automatic prompt discovery and optimization using algorithms such as Reprompting. Acknowledgements Wethank Bill Dolan, Sudha Rao and the reviewers for their valuable feedback. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Arora, S., Narayan, A., Chen, M. F., Orr, L. J., Guha, N., Bhatia, K., Chami, I., Sala, F., and R√©, C. Ask me anything: A simple strategy for prompting language models. arXiv preprint arXiv:2210.02441, 2022. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. Neural Information Processing Systems (NeurIPS), 2020. Casella, G. and George, E. I. Explaining the gibbs sampler. The American Statistician, 46(3):167‚Äì174, 1992. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021. URL https://arxiv. org/abs/2107.03374. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Creswell, A., Shanahan, M., and Higgins, I. Selectioninference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022. Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E. P., and Hu, Z. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548, 2022. Fu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot, T. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022. Geman, S. and Geman, D. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-6:721‚Äì741, 1984. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the MATH dataset. CoRR, abs/2103.03874, 2021. URL https://arxiv. org/abs/2103.03874. Jojic, A., Wang, Z., and Jojic, N. Gpt is becoming a turing machine: Here are some ways to program it, 2023. 9Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and Chen, W. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022. Liu, Z., Patwary, M., Prenger, R., Prabhumoye, S., Ping, W., Shoeybi, M., and Catanzaro, B. Multi-stage prompting for knowledgeable dialogue generation. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 1317‚Äì1337, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. f indings-acl.104. URL https://aclanthology. org/2022.findings-acl.104. Madaan, A. and Yazdanbakhsh, A. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022. Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. OpenAI. Gpt-4 technical report, 2023. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., and Ribeiro, M. T. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023. Perez, E., Kiela, D., and Cho, K. True few-shot learning with language models. Advances in neural information processing systems, 34:11054‚Äì11070, 2021. Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. Pryzant, R., Iter, D., Li, J., Lee, Y., Zhu, C., and Zeng, M. Automatic prompt optimization with ‚Äúgradient descent‚Äù and beam search. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 7957‚Äì7968, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.494. URL https://aclanthology. org/2023.emnlp-main.494. Roberts, G. O. and Smith, A. F. Simple conditions for the convergence of the gibbs sampler and metropolis-hastings algorithms. Stochastic processes and their applications, 49(2):207‚Äì216, 1994. Schick, T. and Sch√ºtze, H. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676, 2020. Shwartz, V., West, P., Le Bras, R., Bhagavatula, C., and Choi, Y. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4615‚Äì4629, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.373. URL https:// aclanthology.org/2020.emnlp-main.373. Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Suzgun, M., Scales, N., Sch√§rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022a. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824‚Äì24837, 2022. Yoran, O., Wolfson, T., Bogin, B., Katz, U., Deutch, D., and Berant, J. Answering questions by metareasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007, 2023. Zamfirescu-Pereira, J., Wong, R. Y., Hartmann, B., and Yang, Q. Why johnny can‚Äôt prompt: How non-ai experts try (and fail) to design llm prompts. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI ‚Äô23, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450394215. doi: 10.1145/3544548.3581388. URL https://doi. org/10.1145/3544548.3581388. 10Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling Zelikman, E., Wu, Y., and Goodman, N. D. STaR: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465, 2022. Zhang, T., Wang, X., Zhou, D., Schuurmans, D., and Gonzalez, J. E. TEMPERA: Test-time prompt editing via reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=gSHyqBijPFO. Zhang, Z., Zhang, A., Li, M., and Smola, A. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. Zheng, C., Liu, Z., Xie, E., Li, Z., and Li, Y. Progressivehint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023. Zhou, D., Sch√§rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=92gvk82DE-. 11Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling Figure A.1: Comparing the CoT recipes inferred through Reprompting using IntructGPT alone versus ChatGPT (for initialization) + InstructGPT (for sampling). A. Additional Illustrations Onsensitivity to initialization We have shown that Reprompting can be sensitive to initial recipe generation. Armed with the optimal prompts discovered with ChatGPT+InstructGPT through Reprompting, InstructGPT can reach test accuracy equalling or besting ChatGPT on most challenging reasoning tasks. However, on some tasks, such prompts could not be discovered using InstructGPT itself as the initialization model LLM1. Figure A.1 points to a likely explanation: ChatGPT can generate a wider range of useful recipes, and whether these initial recipes lead to the correct solution or not, InstructGPT can follow them and, through Reprompting, refine and correct them iteratively. Thus, as we have shown in our experiments, with a diverse pool of initial recipes, LLMs that may appear inferior based on their zero-shot performance may end up performing just as well or better than LLMs whose zero-shot performance is more encouraging. It would be interesting to see if Reprompting can use a mixture of LLMs in initialization to perform even better, or if humans can be put back into the loop to provide some initial recipes or some generic instructions on how to generate such recipes. On transferability of discovered recipes The fact that LLM1 (ChatGPT) can point LLM2 (InstructGPT) in the right directions for prompt discovery does not mean that the discovered prompts, having been optimized for training performance on LLM2, will perform well when used to prompt LLM1. In fact, Table 4 indicates that the discovered CoT recipes that work for one model may not necessarily work for other models. For example, in the case of Temporal Sequences, the best performance is achieved with a prompt trained with InstructGPT (after initialization with ChatGPT as LLM1). But when using that prompt on ChatGPT, the test performance is by 18% lower. Figure A.2 illustrates how ChatGPT and InstructGPT follow the same CoT prompt differently. Following the prompt recipes, the time intervals that need to be reasoned over are sorted, and among the sorted list, the missing interval was inserted as the possible interval when the person in question could have performed an activity. InstructGPT follows this procedure with accuracy over 99%, but ChatGPT sometimes skips the crucial line (for this recipe) with the missing interval within the timeline and therefore obtains suboptimal test accuracy. However, the best performance of ChatGPT (using the CoT prompt optimized for itself through Reprompting) is only slightly lower than that of the ChatGPT+InstructGPT combination. These results suggest that, for a fair comparison between different LLMs, one needs to optimize the CoT prompt for each LLMusing prompt optimization algorithms such as Reprompting. 12Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling Figure A.2: An example on Temporal Sequences (BBH) where ChatGPT underperforms InstructGPT using the same CoT prompt optimized for InstructGPT via Reprompting (using ChatGPT+InstructGPT). ChatGPT fails to correctly execute the recipe as it skips a key step (the blue underlined text from InstructGPT) to reach the final answer. (The illustration does not show the full prompt that precedes the puzzle x for brevity; it consists of 5 training examples with worked-out solutions that all follow the same strategy of solving these types of problems.) Howdothemodel-generated CoTrecipes differ from human-written ones? Inthepaper, We evaluated the performance of the CoT prompt discovered through Reprompting and contrasted it with human-written ones. As illustrated by the example recipes in Figure A.3, the automatically discovered CoT recipes share some similarities to human-written ones on some tasks (such as Logical Deduction), but differs on other tasks. For instance, on Object Counting, the CoT generated using Reprompting computes the total number of objects by incrementing the count one by one (e.g. adding 4 to the count 5 by ‚Äú[6,7,8,9]‚Äù), while in the human written recipe, it computes the addition through an arithmetic formula at the end. 13Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling (a) (b) Figure A.3: Examples of the best-performing CoT recipes inferred via Reprompting on Logical Deduction, Geometric Shapes, Object Counting, Penguins in a Table, and Temporal Sequences.

Reflexion: Language Agents with Verbal Reinforcement Learning Noah Shinn Northeastern University noahshinn024@gmail.com arXiv:2303.11366v4  [cs.AI]  10 Oct 2023 Edward Berman Northeastern University berman.ed@northeastern.edu Karthik Narasimhan Princeton University karthikn@princeton.edu Federico Cassano Northeastern University cassano.f@northeastern.edu Ashwin Gopinath Massachusetts Institute of Technology agopi@mit.edu Shunyu Yao Princeton University shunyuy@princeton.edu Abstract Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is f lexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance. We release all code, demos, and datasets at https://github.com/noahshinn024/reflexion. 1 Introduction Recent works such as ReAct [30], SayCan [1], Toolformer [22], HuggingGPT [23], generative agents [19], and WebGPT [17] have demonstrated the feasibility of autonomous decision-making agents that are built on top of a large language model (LLM) core. These methods use LLMs to generate text and ‚Äòactions‚Äò that can be used in API calls and executed in an environment. Since they rely on massive models with an enormous number of parameters, such approaches have been so far limited to using in-context examples as a way of teaching the agents, since more traditional optimization schemes like reinforcement learning with gradient descent require substantial amounts of compute and time. Preprint. Under review.In this paper, we propose an alternative approach called Reflexion that uses verbal reinforcement to help agents learn from prior failings. Reflexion converts binary or scalar feedback from the environment into verbal feedback in the form of a textual summary, which is then added as additional context for the LLM agent in the next episode. This self-reflective feedback acts as a ‚Äòsemantic‚Äô gradient signal by providing the agent with a concrete direction to improve upon, helping it learn from prior mistakes to perform better on the task. This is akin to how humans iteratively learn to accomplish complex tasks in a few-shot manner‚Äì by reflecting on their previous failures in order to form an improved plan of attack for the next attempt. For example, in figure 1, a Reflexion agent learns to optimize its own behavior to solve decision-making, programming, and reasoning tasks through trial, error, and self-reflection. Generating useful reflective feedback is challenging since it requires a good understanding of where the model made mistakes (i.e. the credit assignment problem [25]) as well as the ability to generate a summary containing actionable insights for improvement. We explore three ways for doing this‚Äì simple binary environment feedback, pre-defined heuristics for common failure cases, and self-evaluation such as binary classification using LLMs (decision-making) or self-written unit tests (programming). In all implementations, the evaluation signal is amplified to natural language experience summaries which can be stored in long-term memory. Reflexion has several advantages compared to more traditional RL approaches like policy or valuebased learning: 1) it is lightweight and doesn‚Äôt require finetuning the LLM, 2) it allows for more nuanced forms of feedback (e.g. targeted changes in actions), compared to scalar or vector rewards that are challenging to perform accurate credit assignment with, 3) it allows for a more explicit and interpretable form of episodic memory over prior experiences, and 4) it provides more explicit hints for actions in future episodes. At the same time, it does have the disadvantages of relying on the power of the LLM‚Äôs self-evaluation capabilities (or heuristics) and not having a formal guarantee for success. However, as LLM capabilities improve, we only expect this paradigm to get better over time. We perform experiments on (1) decision-making tasks to test sequential action choices over long trajectories, (2) reasoning tasks to test knowledge-intensive, single-step generation improvement, and (3) programming tasks to teach the agent to effectively use external tools such as compilers and interpreters. Across all three types of tasks, we observe Reflexion agents are better decisionmakers, reasoners, and programmers. More concretely, Reflexion agents improve on decision-making AlfWorld [24] tasks over strong baseline approaches by an absolute 22% in 12 iterative learning steps, and on reasoning questions in HotPotQA [28] by 20%, and Python programming tasks on HumanEval [6] by as much as 11%. To summarize, our contributions are the following: ‚Ä¢ We propose Reflexion, a new paradigm for ‚Äòverbal‚Äò reinforcement that parameterizes a policy as an agent‚Äôs memory encoding paired with a choice of LLM parameters. ‚Ä¢ We explore this emergent property of self-reflection in LLMs and empirically show that self-reflection is extremely useful to learn complex tasks over a handful of trials. ‚Ä¢ Weintroduce LeetcodeHardGym, a code-generation RL gym environment consisting of 40 challenging Leetcode questions (‚Äòhard-level‚Äò) in 19 programming languages. ‚Ä¢ Weshowthat Reflexion achieves improvements over strong baselines across several tasks, and achieves state-of-the-art results on various code generation benchmarks. 2 Related work Reasoning and decision-making Self-Refine [15] employs an iterative framework for selfrefinement to autonomously improve generation through self-evaluation. These self-evaluation and self-improvement steps are conditioned on given task constraints, such as "How can this generation be written in a more positive way". Self-Refine is effective but is limited to single-generation reasoning tasks. Pryzant et al. [21] performs a similar semantic prompt-writing optimization, but is also limited to single-generation tasks. Paul et al. [20] fine-tune critic models to provide intermediate feedback within trajectories to improve reasoning responses. Xie et al. [27] use stochastic beam search over actions to perform a more efficient decision-making search strategy which allows the agent to use foresight advantage due to its self-evaluation component. Yoran et al. [31] and Nair et al. 2Figure1:Reflexionworksondecision-making4.1,programming4.3,andreasoning4.2tasks. Relatedworkonreasoninganddecision-making Approach Self Hidden Decision Binary Memory refine constraints making reward Self-refine[15] ‚úì ‚úó ‚úó ‚úó ‚úó Beamsearch[27] ‚úì ‚úì ‚úì ‚úì ‚úó Reflexion(ours) ‚úì ‚úì ‚úì ‚úì ‚úì Relatedworkonprogramming Approach Test Debugging Self-generated Multiple Self-reflection Testexecution execution tests languages AlphaCode[14] ‚úì ‚úó ‚úó ‚úì ‚úó CodeT[5] ‚úì ‚úó ‚úì ‚úó ‚úó Self-debugging[7] ‚úì ‚úì ‚úó ‚úó ‚úó CodeRL[12] ‚úì ‚úì ‚úó ‚úó ‚úó Reflexion(ours) ‚úì ‚úì ‚úì ‚úì ‚úì [16]usedecidermodelstoreasonoverseveralgenerations.Kimetal.[10]usearetrypatternover afixednumberofstepswithoutanevaluationstep.Goodman[9]performaqualitativeevaluation stepthatproposesoptimizationstothepreviousgeneration. Inthispaper,weshowthatseveralof theseconceptscanbeenhancedwithself-reflectiontobuildapersistingmemoryofself-reflective experienceswhichallowsanagenttoidentifyitsownerrorsandself-suggestlessonstolearnfromits mistakesovertime. Programming Severalpastandrecentworksemployvariationsof test-drivendevelopmentor codedebuggingpractices. AlphaCode[14]evaluatesasetofgenerationsonhiddentestcases. CodeT[5]usesself-generatedunitteststhatareusedtoscoregeneratedfunctionimplementations. Self-Debugging[7]employsadebuggingcomponentthatisusedtoimproveexistingimplementations givenfeedbackfromacodeexecutionenvironment.CodeRL[12]setstheprobleminanRLframeworkusinganactor-criticsetuptodebugprogramsgivenfeedbackfromanexecutionenvironment. AlphaCode,Self-DebuggingandCodeRLareeffectiveinfixingless-complexprogrambugs,butthey relyupongroundtruthtestcasesthatinvalidatepass@1eligibility,anddonotuseself-reflectionto bridgethegapbetweenerroridentificationandimplementationimprovement.CodeTdoesnotaccess hiddentestcasesbutdoesnotimplementaself-learningsteptoimprovecodewriting. 3 Reflexion:reinforcementviaverbalreflection WedevelopamodularformulationforReflexion,utilizingthreedistinctmodels:anActor,denotedas Ma,whichgeneratestextandactions;anEvaluatormodel,representedbyMe,thatscorestheoutputs producedbyMa;andaSelf-Reflectionmodel,denotedasMsr,whichgeneratesverbalreinforcement cuestoassist theActorinself-improvement.Weprovideadetaileddescriptionofeachofthese modelsandsubsequentlyelucidatetheircollaborativefunctioningwithintheReflexionframework. 3Agent External feedback Internal feedback Self-reflection (LM) Evaluator (LM) Trajectory (short-term memory) Experience (long-term memory) Obs / Reward Environment Algorithm 1 Reinforcement via self-reflection Initialize Actor, Evaluator, Self-Reflection: Ma, Me, Msr Reflective text Actor (LM) Action Initialize policy œÄŒ∏(ai|si), Œ∏ = {Ma,mem} Generate initial trajectory using œÄŒ∏ Evaluate œÑ0 using Me Generate initial self-reflection sr0 using Msr Set mem ‚Üê[sr0] Set t = 0 while Me not pass or t < max trials do Generate œÑt = [a0,o0,...ai,oi] using œÄŒ∏ Evaluate œÑt using Me Generate self-reflection srt using Msr Append srt to mem Increment t end while return Figure 2: (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm Actor The Actor is built upon a large language model (LLM) that is specifically prompted to generate the necessary text and actions conditioned on the state observations. Analogous to traditional policy-based RL setups, we sample an action or generation, at, from the current policy œÄŒ∏ at time t, receive an observation from the environment ot. We explore various Actor models, including Chain of Thought [26] and ReAct [30]. These diverse generation models allow us to explore different aspects of text and action generation within the Reflexion framework, providing valuable insights into their performance and effectiveness. In addition, we also add a memory component mem that provides additional context to this agent. This adaption was inspired by Brooks et al. [3], who suggest a policy iteration approach using in-context learning. Details on how this is populated are provided below. Evaluator The Evaluator component of the Reflexion framework plays a crucial role in assessing the quality of the generated outputs produced by the Actor. It takes as input a generated trajectory and computes a reward score that reflects its performance within the given task context. Defining effective value and reward functions that apply to semantic spaces is difficult, so we investigate several variants of the Evaluator model. For reasoning tasks, we explore reward functions based on exact match (EM) grading, ensuring that the generated output aligns closely with the expected solution. In decision-making tasks, we employ pre-defined heuristic functions that are tailored to specific evaluation criteria. Additionally, we experiment with using a different instantiation of an LLMitself as an Evaluator, generating rewards for decision-making and programming tasks. This multi-faceted approach to Evaluator design allows us to examine different strategies for scoring generated outputs, offering insights into their effectiveness and suitability across a range of tasks. Self-reflection The Self-Reflection model instantiated as an LLM, plays a crucial role in the Reflexion framework by generating verbal self-reflections to provide valuable feedback for future trials. Given a sparse reward signal, such as a binary success status (success/fail), the current trajectory, and its persistent memory mem, the self-reflection model generates nuanced and specific feedback. This feedback, which is more informative than scalar rewards, is then stored in the agent‚Äôs memory (mem). For instance, in a multi-step decision-making task, when the agent receives a failure signal, it can infer that a specific action ai led to subsequent incorrect actions ai+1 and ai+2. The agent can then verbally state that it should have taken a different action, a‚Ä≤ i, which would have resulted in a‚Ä≤ i+1 and a‚Ä≤ i+2, and store this experience in its memory. In subsequent trials, the agent can leverage its past experiences to adapt its decision-making approach at time t by choosing action a‚Ä≤ i. This iterative process of trial, error, self-reflection, and persisting memory enables the agent to rapidly improve its decision-making ability in various environments by utilizing informative feedback signals. Memory Core components of the Reflexion process are the notion of short-term and long-term memory. At inference time, the Actor conditions its decisions on short and long-term memory, similar 4to the way that humans remember fine-grain recent details while also recalling distilled important experiences from long-term memory. In the RL setup, the trajectory history serves as the short-term memory while outputs from the Self-Reflection model are stored in long-term memory. These two memory components work together to provide context that is specific but also influenced by lessons learned over several trials, which is a key advantage of Reflexion agents over other LLM action choice works. The Reflexion process Reflexion is formalized as an iterative optimization process in 1. In the f irst trial, the Actor produces a trajectory œÑ0 by interacting with the environment. The Evaluator then produces a score r0 which is computed as rt = Me(œÑ0). rt is only a scalar reward for trial t that improves as task-specific performance increases. After the first trial, to amplify r0 to a feedback form that can be used for improvement by an LLM, the Self-Reflection model analyzes the set of {œÑ0,r0} to produce a summary sr0 which is stored in the memory mem. srt is a verbal experience feedback for trial t. The Actor, Evaluator, and Self-Reflection models work together through trials in a loop until the Evaluator deems œÑt to be correct. As mentioned in 3, the memory component of Reflexion is crucial to its effectiveness. After each trial t, srt, is appended mem. In practice, we bound mem by a maximum number of stored experiences, ‚Ñ¶ (usually set to 1-3) to adhere to max context LLM limitations. 4 Experiments Weevaluate various natural language RL setups on decision-making, reasoning, and code generation tasks. Specifically, we challenge an agent to perform search-based question answering on HotPotQA [28], multi-step tasks in common household environments in AlfWorld [24], and code writing tasks in competition-like environments with interpreters and compilers in HumanEval [6], MBPP [2], and LeetcodeHard, a new benchmark. Most notably, Reflexion improves performance over strong baselines by 22% in AlfWorld, 20% in HotPotQA, and 11% on HumanEval. 4.1 Sequential decision making: ALFWorld AlfWorld is a suite of text-based environments that challenge an agent to solve multi-step tasks in a variety of interactive environments based on TextWorld [8]. Following Yao et al. [30], we run the agent in 134 AlfWorld environments across six different tasks, including finding hidden objects (e.g., finding a spatula in a drawer), moving objects (e.g., moving a knife to the cutting board), and manipulating objects with other objects (e.g., chilling a tomato in the fridge). We use ReAct [30] as the action generator as Yao et al. [30] has shown success in long trajectory decisionmaking using explicit intermediate thoughts. AlfWorld tasks naturally require a self-evaluation step as the environment can only signal if a task is complete. To achieve fully autonomous behavior, we implement two self-evaluation techniques: natural language classification using an LLM and a hand-written heuristic. The heuristic is simple: if the agent executes the same action and receives the same response for more than 3 cycles, or if the number of actions taken in the current environment exceeds 30 (inefficient planning), we self-reflect. In the baseline runs, if self-reflection is suggested, we skip the self-reflection process, reset the environment, and start a new trial. In the Reflexion runs, the agent uses self-reflection to find its mistake, update its memory, reset the environment, and start a new trial. To avoid very long prompt windows that may exceed the maximum limit, we truncate the agent‚Äôs memory to the last 3 self-reflections (experiences). To avoid syntactic errors, we provide two domain-specific few-shot trajectories to the agent. We use the same few-shot trajectory examples as Yao et al. [30] with GPT-3 for the LLM. AlfWorld tasks, ReAct few-shot prompts, and Reflexion examples are included in the appendix. Results ReAct + Reflexion significantly outperforms ReAct by completing 130 out of 134 tasks using the simple heuristic to detect hallucinations and inefficient planning. Further, ReAct + Reflexion learns to solve additional tasks by learning in 12 consecutive trials. In the ReAct-only approach, we see that performance increase halts between trials 6 and 7. Analysis A common error in baseline failed AlfWorld trajectories is when an agent thinks that it has possession of an item but does not actually have the item. The agent proceeds to execute several actions in a long trajectory and is not able to backtrack its actions to find the mistake. Reflexion 51.0 (a) ALFWorld Success Rate 0.5 (a) ALFWorld Success Rate Proportion of Solved Environments ReAct only ReAct + Reflexion (Heuristic) ReAct + Reflexion (GPT) 0.9 0.8 0.7 0.6 0.5 0 2 4 6 Trial Number 8 10 ReAct only - hallucination ReAct only - inefficient planning ReAct + Reflexion - hallucination 0.4 0.3 0.2 Proportion of Environments 0.1 0.0 ReAct + Reflexion - inefficient planning 0 2 4 6 Trial Number 8 10 Figure 3: (a) AlfWorld performance across 134 tasks showing cumulative proportions of solved tasks using self-evaluation techniques of (Heuristic) and (GPT) for binary classification. (b) Classification of AlfWorld trajectories by reason of failure. eliminates almost all of these cases by using self-reflection to distill long, failed trajectories into relevant experiences that can are used as "self-hints" in the future. There are two main cases in which long-term memory helps an agent in AlfWorld: 1) An early mistake in a long trajectory can be easily identified. The agent can suggest a new action choice or even a new long-term plan. 2) There are too many surfaces/containers to check for an item. The agent can exploit its experience memory over several trials to thoroughly search a room. In 3, the learning curve suggests that the learning process occurs over several experiences, meaning that the agent is successfully balancing cases 1 and 2 shown in the immediate spike in the improvement between the first two trials, then a steady increase over the next 11 trials to a near-perfect performance. On the other hand, 3 shows a ReAct-only agent converging at a hallucination rate of 22% with no signs of long-term recovery. 4.2 Reasoning: HotpotQA HotPotQA [28] is a Wikipedia-based dataset with 113k question-and-answer pairs that challenge agents to parse content and reason over several supporting documents. To test improvement in reasoning only ability, we implement Reflexion + Chain-of-Thought (CoT) [26] for step-by-step Q‚ÜíAandQ,Cgt ‚ÜíAimplementations, where Q is the question, Cgt is the ground truth context from the dataset, and A is the final answer. Since CoT is not a multi-step decision-making technique, we give Cgt to the agent so that we can isolate the reasoning behavior over large sections of the provided text. To test holistic question and answering ability, which requires reasoning and action choice, we implement a Reflexion + ReAct [30] agent that can retrieve relevant context using a Wikipedia API and infer answers using step-by-step explicit thinking. For CoT implementations, we use 6-shot prompting; for ReAct, we use 2-shot prompting, and for self-reflection, we use 2-shot prompting. All examples can be found in the appendix. Robustly evaluating natural language answers is a long-standing problem in NLP. Therefore, between trials, we use exact match answer grading using the environment to give a binary success signal to the agent. After each trial, the self-reflection loop is employed to amplify the binary signal, similar to the decision-making setup 4.1 in AlfWorld with a memory size of 3 experiences. Results Reflexion outperforms all baseline approaches by significant margins over several learning steps. Furthermore, ReAct-only, CoT-only, and CoT (GT)-only implementations fail to probabilistically improve on any tasks, meaning that no failed tasks from the first trial from any of the baseline approaches were able to be solved in subsequent trials using a temperature of 0.7 In the Reflexion runs, we allowed the agent to gather experience and retry on failed tasks until it produced 3 consecutive failed attempts on the particular task. Naturally, the CoT (GT) achieved higher accuracy scores as it was given access to the ground truth context of the question. Still, the CoT (GT) agent is unable to correctly infer the correct answer for 39% of the questions, but Reflexion helps the agent to correct its mistakes without access to the ground truth answer to improve its accuracy by 14%. 6Proportion of Solved Tasks 0.8 (a) HotPotQA Success Rate 1.0 (b) HotPotQA CoT (GT) (c) HotPotQA Episodic Memory CoT (GT) only CoT only ReAct only 0.6 0.4 0.2 CoT + Reflexion ReAct + Reflexion 0 2 Trial Number CoT (GT) + Reflexion 0.8 0.6 0.4 Proportion of Solved Tasks 4 6 0 1 2 3 4 5 6 7 Trial Number 1.0 0.9 0.8 0.7 0.6 Proportion of Solved Tasks 0.5 CoT (GT) only CoT (GT) EPM CoT (GT) EPM + Reflexion 0 1 3 2 Trial Number 4 Figure 4: Chain-of-Thought (CoT) and ReAct. Reflexion improves search, information retrieval, and reasoning capabilities on 100 HotPotQA questions. (a) Reflexion ReAct vs Reflexion CoT (b) Reflexion CoT (GT) for reasoning only (c) Reflexion vs episodic memory ablation. Analysis We perform an ablation experiment to isolate the advantage of the self-reflective step for reasoning using CoT (GT) as the baseline approach 4. Recall that CoT (GT) uses Chain-of-Thought reasoning with provided ground truth context, which tests reasoning ability over long contexts. Next, we add an element of episodic memory (EPM) by including the most recent trajectory. For the Reflexion agent, we implement the standard self-reflection step as a final pass. Intuitively, we test if the agent is iteratively learning more effectively by using verbal explanation using language written in the first person. 4 shows that self-reflection improves learning by an 8% absolute boost over the episodic memory learning advantage. This result supports the argument that refinement-only approaches are not as effective as self-reflection-guided refinement approaches. 4.3 Programming We evaluate the baseline and Reflexion approaches on Python and Rust code writing on MBPP [2], HumanEval [6], and LeetcodeHardGym, our new dataset. MBPP and HumanEval measure function body generation accuracy given natural language descriptions. We use a benchmark language compiler, MultiPL-E [4], to translate subsets of HumanEval and MBPP to the Rust language. MultiPLE is a collection of small compilers that can be used to translate Python benchmark questions to 18 other languages. We include experiments for Rust code generation to demonstrate that Reflexion implementations for code generation are language-agnostic and can be used for interpreted and compiled languages. Lastly, we introduce a new benchmark, LeetcodeHardGym, which is an interactive programming gym that contains 40 Leetcode hard-rated questions that have been released after October 8, 2022, which is the pre-training cutoff date of GPT-4 [18]. Thetask of programming presents a unique opportunity to use more grounded self-evaluation practices such as self-generated unit test suites. Thus, our Reflexion-based programming task implementation is eligible for pass@1 accuracy reporting. To generate a test suite, we use Chain-of-Thought prompting [26] to produce diverse, extensive tests with corresponding natural language descriptions. Then, we f ilter for syntactically valid test statements by attempting to construct a valid abstract syntax tree (AST) for each proposed test. Finally, we sample n tests from the collection of generated unit tests to produce a test suite T, denoted as {t0,t1,...,tn}. We set n to a maximum of 6 unit tests. Aside from the unit test suite component, the setup for the learning loop for a Reflexion programming agent is identical to the reasoning and decision-making agents with a max memory limit of 1 experience. Benchmark + Language Prev SOTAPass@1 SOTAPass@1 ReflexionPass@1 HumanEval (PY) HumanEval (RS) MBPP(PY) MBPP(RS) Leetcode Hard (PY) 65.8 (CodeT [5] + GPT-3.5)‚Äì 80.1 (GPT-4) 60.0 (GPT-4) 67.7 (CodeT [5] + Codex [6]) 80.1 (GPT-4)‚Äì‚Äì 70.9 (GPT-4) 7.5 (GPT-4) 91.0 68.0 77.1 75.4 15.0 Table 1: Pass@1 accuracy for various model-strategy-language combinations. The base strategy is a single code generation sample. All instruction-based models follow zero-shot code generation. 7Benchmark + Language Base Reflexion TP FN FP TN HumanEval (PY) MBPP(PY) HumanEval (RS) MBPP(RS) 0.80 0.80 0.60 0.71 0.91 0.77 0.68 0.75 0.99 0.40 0.01 0.60 0.84 0.59 0.16 0.41 0.87 0.37 0.13 0.63 0.84 0.51 0.16 0.49 Table 2: Overall accuracy and test generation performance for HumanEval and MBPP. For Rust, HumanEval is the hardest 50 problems from HumanEval Python translated to Rust with MultiPL-E [4]. TP: unit tests pass, solution pass; FN: unit tests fail, solution pass; FP: unit tests pass, solution fail; TN: unit tests fail, solution fail. Results Reflexion outperforms all baseline accuracies and sets new state-of-the-art standards on all benchmarks for Python and Rust except for MBPP Python 1. We further investigate the inferior performance of Reflexion on MBPP Python. Analysis We acknowledge that self-reflecting code-generation agents are bound to their ability to write diverse, comprehensive tests. Therefore, in the case in which the model generates a flaky test suite, it is possible that all tests pass on an incorrect solution and lead to a false positive label on a code completion [11]. On the other hand, if the model produces an incorrectly written test suite, it is possible for some of the tests to fail on a correct solution, leading to a self-reflection generation that is conditioned on a false negative code completion. Given the implementation of Reflexion, false negatives are preferred over false positives as the agent may be able to use self-reflection to identify the incorrect test(s) and prompt itself to keep the original code completion intact. On the other hand, if an invalid test suite returns a false positive completion (all internal test cases pass but the implementation is incorrect), the agent will prematurely report an invalid submission. In 2, various conditions are measured to analyze performance beyond pass@1 accuracy. Previously, we displayed the inferior performance of Reflexion to the baseline GPT-4 on MBPP Python. In 2, we observe a notable discrepancy between the false positive labels produced by internal test execution, P(not pass@1 generation correct | tests pass). That is, the probability that a submission will fail given that it passes all unit tests. For HumanEval and MBPP Python, the baseline pass@1 accuracies are relatively similar, 82% and 80%, respectively. However, the false positive test execution rate for MBPPPython is 16.3% while the rate for HumanEval Python is a mere 1.4%, leading to 91% overall accuracy 1. Approach Test Generation Self-reflection Pass@1 (Acc) Base model False Test generation omission False Self-reflection omission Reflexion True True False True False True 0.60 0.52 0.60 0.68 Table 3: Pass@1 accuracy for various compromised approaches on the Reflexion approach using GPT-4 as the base model on HumanEval Rust- 50 hardest problems Ablation study We test the composite approach of Reflexion for test generation and self-reflection cooperation on a subset of the 50 hardest HumanEval Rust problems. Our Rust compiler environment provides verbose error logs and helpful debugging hints, therefore serving as a good playground for compromised approaches. First, we omit internal test generation and execution steps, which test the agent to self-reflect without guidance from current implementations. 3 shows an inferior 52% vs 60%(baseline) accuracy, which suggests that the agent is unable to determine if the current implementation is correct without unit tests. Therefore, the agent must participate in all iterations of the run without the option to return early, performing harmful edits to the implementation. Next, we test self-reflection contribution by omitting the natural language explanation step following failed unit test suite evaluations. Intuitively, this challenges the agent to combine the tasks of error identification and implementation improvement across all failed unit tests. Interestingly, the compromised agent does not improve performance over the baseline run. We observe that the test generation and code compilation steps are able to catch syntax and logic errors, but the implementation f ixes do not reflect these indications. These empirical results suggest that several recent works that 8propose blind trial and error debugging techniques without self-reflection are ineffective on harder tasks such as writing complex programs in Rust. 5 Limitations At its core, Reflexion is an optimization technique that uses natural language to do policy optimization. Policy optimization is a powerful approach to improve action choice through experience, but it may still succumb to non-optimal local minima solutions. In this study, we limit long-term memory to a sliding window with maximum capacity, but we encourage future work to extend the memory component of Reflexion with more advanced structures such as vector embedding databases or traditional SQL databases. Specific to code generation, there are many practical limitations to testdriven development in specifying accurate input-output mappings such as non-deterministic generator functions, impure functions that interact with APIs, functions that vary output according to hardware specifications, or functions that invoke parallel or concurrent behavior that may be difficult to predict. 6 Broaderimpact Large language models are increasingly used to interact with external environments (e.g.the Internet, software, robotics, etc.) and humans. Our work has the potential of reinforcing and empowering these agents toward greater automation and work efficiency, but it also amplifies the risks when these agents were put into misuse. We believe that this direction of research will need more effort in safety and ethical considerations. On the other hand, reinforcement learning has suffered from its black-box policy and optimization setups in which interpretability and alignment have been challenging. Our proposed ‚Äúverbal‚Äù reinforcement learning might address some of the issues and turn autonomous agents more interpretable and diagnosable. For example, in the case of tool-usage that may be too hard for humans to understand, self-reflections could be monitored to ensure proper intent before using the tool. 7 Conclusion In this work, we present Reflexion, an approach that leverages verbal reinforcement to teach agents to learn from past mistakes. We empirically show that Reflexion agents significantly outperform currently widely-used decision-making approaches by utilizing self-reflection. In future work, Reflexion could be used to employ more advanced techniques that have been thoroughly studied in traditional RL settings, such as value learning in natural language or off-policy exploration techniques. 8 Reproducibility We highly advise others to use isolated execution environments when running autonomous code writing experiments as the generated code is not validated before execution. 9References [1] Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., et al. (2022). Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. [2] Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. (2021). Program synthesis with large language models. arXiv preprint arXiv:2108.07732. [3] Brooks, E., Walls, L., Lewis, R. L., and Singh, S. (2022). In-context policy iteration. arXiv preprint arXiv:2210.03821. [4] Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps-Costin, L., Pinckney, D., Yee, M.-H., Zi, Y., Anderson, C. J., Feldman, M. Q., Guha, A., Greenberg, M., and Jangda, A. (2022). Multipl-e: Ascalable and extensible approach to benchmarking neural code generation. [5] Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.-G., and Chen, W. (2022). Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397. [6] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. (2021). Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. [7] Chen, X., Lin, M., Sch√§rli, N., and Zhou, D. (2023). Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128. [8] C√¥t√©, M.-A., K√°d√°r, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., El Asri, L., Adada, M., et al. (2019). Textworld: A learning environment for text-based games. In Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7, pages 41‚Äì75. Springer. [9] Goodman, N. (2023). Meta-prompt: A simple self-improving language agent. noahgoodman.substack.com. [10] Kim, G., Baldi, P., and McAleer, S. (2023). Language models can solve computer tasks. arXiv preprint arXiv:2303.17491. [11] Lam, W., Winter, S., Wei, A., Xie, T., Marinov, D., and Bell, J. (2020). A large-scale longitudinal study of flaky tests. Proc. ACM Program. Lang., 4(OOPSLA). [12] Le, H., Wang, Y., Gotmare, A. D., Savarese, S., and Hoi, S. C. H. (2022). Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314‚Äì21328. [13] Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. (2023). Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161. [14] Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. (2022). Competition-level code generation with alphacode. Science, 378(6624):1092‚Äì1097. [15] Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. (2023). Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651. [16] Nair, V., Schumacher, E., Tso, G., and Kannan, A. (2023). Dera: Enhancing large language model completions with dialog-enabled resolving agents. arXiv preprint arXiv:2303.17071. [17] Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. (2021). Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332. [18] OpenAI (2023). Gpt-4 technical report. ArXiv. 10[19] Park, J. S., O‚ÄôBrien, J. C., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442. [20] Paul, D., Ismayilzada, M., Peyrard, M., Borges, B., Bosselut, A., West, R., and Faltings, B. (2023). Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904. [21] Pryzant, R., Iter, D., Li, J., Lee, Y. T., Zhu, C., and Zeng, M. (2023). Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495. [22] Schick, T., Dwivedi-Yu, J., Dess√¨, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. (2023). Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761. [23] Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. (2023). Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580. [24] Shridhar, M., Yuan, X., C√¥t√©, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. (2021). ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR). [25] Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. The MIT Press, second edition. [26] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. (2022). Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903. [27] Xie, Y., Kawaguchi, K., Zhao, Y., Zhao, X., Kan, M.-Y., He, J., and Xie, Q. (2023). Decomposition enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633. [28] Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. (2018). HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP). [29] Yao, S., Chen, H., Yang, J., and Narasimhan, K. (preprint). Webshop: Towards scalable real-world web interaction with grounded language agents. In ArXiv. [30] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. (2023). ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). [31] Yoran, O., Wolfson, T., Bogin, B., Katz, U., Deutch, D., and Berant, J. (2023). Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007. 11A Evaluation with additional models Wefurther investigated the applicability of trial-and-error problem-solving with models of various strengths. We found that the ability to specify self-corrections is an emergent quality of stronger, larger models. Approach Pass@1 accuracy (avg over 8 trials) Pass@1 accuracy (std) Baseline Reflexion 0.26 0.26 0.00481 0.00305 Table 4: Pass@1 accuracy on HumanEval Python using starchat-beta [13]. Model Baseline accuracy Reflexion accuracy CoT (GT) + text-davinci-003 0.60 CoT (GT) + gpt-3.5-turbo CoT (GT) + gpt-4 ReAct + text-davinci-003 ReAct + gpt-3.5-turbo ReAct + gpt-4 0.57 0.68 0.30 0.26 0.39 0.77 0.71 0.80 0.55 0.38 0.51 Table 5: Pass@1 accuracy on 100 HotPotQA using various models. 12B Decision-making Environment:Youareinthemiddleofaroom.Lookingquicklyaroundyou,youseeabed1,adesk 2,adesk1,adrawer6,adrawer5,adrawer4,adrawer3,adrawer2,adrawer1,agarbagecan1,a laundryhamper1,asafe1,ashelf6,ashelf5,ashelf4,ashelf3,ashelf2,andashelf1. Task:examinethemugwiththedesklamp. Trial#1 >think:Tosolvethetask,Ineedtofindandtakeamug,thenfindanduseadesklamp. >think:FirstIneedtofindamug.Amugismorelikelytoappearindrawer(1-6),desk(1-2),shelf (1-6),garbagecan(1),laundryhamper(1). Icancheckonebyone,startingwithdrawer1. >gotodrawer1 Thedrawer1isclosed. ... >opendrawer6 Youopenthedrawer6.Thedrawer6isopen. Init,youseeakeychain2. >gotodesk1 Onthedesk1,youseeacreditcard3,adesklamp1,alaptop2,amug1,apen1,andapencil1. >think:NowIfindamug(1).Next,Ineedtotakeit. >takemug1fromdesk1 Youpickupthemug1fromthedesk1. >think:NowItakeamug(1).Next,Ineedtofindadesklamp.Adesklampismorelikelytoappear indesk(1-2),sidetable(1-2),shelf(1-6),bed(1),drawer(1-6). Icancheckonebyone,startingwith desk1. >gotodesk1 Nothinghappens. >gotodesk2 Onthedesk2,youseeaalarmclock1,abowl1,amug3,apencil3,andapencil2. >think:NowIfindadesklamp(1).Next,Ineedtouseit. >usedesklamp1 Nothinghappens. >usedesklamp1 Nothinghappens. Status:Fail Reflection:Inthisenvironment,myplanwastofindamugthenfindanduseadesklamp.However, thetasksaystoexaminethemugwiththedesklamp.Ishouldhavelookedforthedesklampfirst,then lookedforthemug.Inoticedthatthedesklampwasfoundondesk1.Inthenexttrial,Iwillgoto desk1,findthelamp,thenlookforthemugandexamineitwiththedesklamp. Trial#2 >gotodesk1 Onthedesk1,youseeacreditcard3,adesklamp1,alaptop2,amug1,apen1,andapencil1. >think:Tosolvethetask,Ineedtofindandtakeamug,thenfindanduseadesklamp. >takemug1fromdesk1 Youpickupthemug1fromthedesk1. >think:Tosolvethetask,Ineedtofindandtakeamug,thenfindanduseadesklamp. >usedesklamp1 Youturnonthedesklamp1. Status:Success Figure5: [Top]AnAlfWorldtrajectoryinwhichtheagentfailedduetoinefficientplanning. Inthe reflection,theagentrecognizesthatitshouldhavelookedforthedesklampthenthemug,notthemug thenthedesklamp. [Bottom]Theagentisabletocorrectitsreasoningtraceandexecuteasequence ofactionsinaconcisemanner. 13B.1 WebShopLimitation In 5, we briefly state that Reflexion struggles to overcome local minima choices that require extremely creative behavior to escape. We observe this shortcoming in an experiment on WebShop [29]. WebShop is a web-based problem-solving benchmark that tests agents to navigate an e-commerce website to locate and purchase products given requests from clients. We test a two-shot ReAct + Reflexion agent in 100 environments. However, after only four trials, we terminate the runs as the agent does not show signs of improvement 6. Further, the agent does not generate helpful, intuitive self-reflections after failed attempts. We conclude that Reflexion is unable to solve tasks that require a significant amount of diversity and exploration. In AlfWorld, the agent is able to adequately explore new environments because the permissible actions can be seen in the observations. In HotPotQA, the agent faces a similar WebShop search query task but is more successful as the search space for Wikipedia articles is more diverse and requires less precise search queries. A common problem for e-commerce search engines is properly handling ambiguity in natural language search interpretations. Thus, WebShop presents a task that requires very diverse and unique behavior from a Reflexion agent. WebShop Success Rate 0.50 Proportion of Solved Environments 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 ReAct only ReAct + Reflexion 0.0 0.5 1.0 1.5 2.0 Trial Number 2.5 3.0 Figure 6: Reflexion vs React performance on WebShop across 100 customer shopping requests. ReAct + Reflexion fails to significantly outperform ReAct. C Programming Programming LLM calls require strict instructions to produce function bodies only, due to the extensive dialogue training of the LLMs. A few programming examples are reported below with instructions high lighted inblue and templates. See the full implementation at https://github. com/noahshinn024/reflexion. C.1 Programming function implementation example (HumanEval Python) Sample function signature: 1 def minSubArraySum(nums): 2 3 4 5 6 """ Given an array of integers nums, find the minimum sum of any non-empty sub-array of nums. Example minSubArraySum([2, 3, 4, 1, 2, 4]) == 1 147 minSubArraySum([-1,-2,-3]) ==-6 8 """ Samplefunctionbodyimplementation: 1 min_sum = float(‚Äôinf‚Äô) 2 for i in range(len(nums)): 3 current_sum = 0 4 for j in range(i, len(nums)): 5 current_sum += nums[j] 6 if current_sum < min_sum: 7 min_sum = current_sum 8 return min_sum C.2 ReflexionActorinstruction YouareaPythonwritingassistant.Youwillbegivenyourpreviousimplementationofafunction, aseriesofunittestsresults,andyourself-reflectiononyourpreviousimplementation.Applythe necessarychangesbelowbyrespondingonlywiththeimprovedbodyofthefunction.Donotinclude thesignatureinyourresponse.Thefirstlineofyourresponseshouldhave4spacesofindentationso thatitfitssyntacticallywiththeuserprovidedsignature.Youwillbegivenafewexamplesbythe user. ReflexionActorgenerationsfollowtheform: (Instruction) (Functionimplementation) (Unittestfeedback) (Self-reflection) (Instructionfornextfunctionimplmentation) Fullexamplesareshowninhttps://github.com/noahshinn024/reflexion C.3 ReflexionSelf-reflectioninstructionandexample YouareaPythonwritingassistant.Youwillbegivenyourpreviousimplementationofafunction, aseriesofunittestsresults,andyourself-reflectiononyourpreviousimplementation.Applythe necessarychangesbelowbyrespondingonlywiththeimprovedbodyofthefunction.Donotinclude thesignatureinyourresponse.Thefirstlineofyourresponseshouldhave4spacesofindentationso thatitfitssyntacticallywiththeuserprovidedsignature.Youwillbegivenafewexamplesbythe user. ReflexionSelf-Reflectiongenerationsfollowtheform: (Instruction) (Functionimplementation) (Unittestfeedback) C.4 ReflexionprogrammingnoSelf-Reflectionablationexample ReflexionnoSelf-ReflectionablationActorgenerationsfollowtheform: (Instruction) (Functionimplementation) (Unittestfeedback) (Self-reflection) (Instructionfornextfunctionimplmentation) C.5 Reflexionprogrammingnotestgenerationablationexample ReflexionnotestgenerationablationActorgenerationsfollowtheform: (Instruction) 15(Function implementation) (Unit test feedback) (Self-reflection) (Instruction for next function implmentation) 16D Reasoning D.1 Fullexample Trial#1:Question:Grown-Upsstarredtheactor whowasbestknownforwhichroleon"‚ÄôAllo ‚ÄôAllo!"? Thought1: IneedtosearchGrown-Upsand "‚ÄôAllo ‚ÄôAllo!", find theactorwhostarred in Grown-Ups,thenfindtherolehewasbestknown forin"‚ÄôAllo‚ÄôAllo!". Action1:Search[Grown-Ups] Observation1: Grown-Ups isa1980British BBCtelevisionfilmdevisedanddirectedby MikeLeigh. It starsLesleyManville, Philip Davis,BrendaBlethyn,JanineDuvitski,Lindsay DuncanandSamKelly. ItwaseditedbyRobin SalesandproducedbyLouisMarksfortheBBC, andoriginallyshownonBBC2on28November 1980. Thought2:Grown-UpsstarredLesleyManville, PhilipDavis,BrendaBlethyn,JanineDuvitski, LindsayDuncanandSamKelly. Ineedtosearch "‚ÄôAllo‚ÄôAllo!"andfindwhichactorwasbest knownforwhichrole. Action2:Search["‚ÄôAllo‚ÄôAllo!"] Observation2:Couldnotfind["‚ÄôAllo‚ÄôAllo!"]. Similar: ["‚ÄôAllo‚ÄôAllo!", "Listof ‚ÄôAllo‚ÄôAllo! characters",‚ÄôGordenKaye‚Äô,"‚ÄôAllo‚ÄôAllo!(series 2)", ‚ÄôAll√¥,all√¥‚Äô, ‚ÄôFrancescaGonshaw‚Äô,"‚ÄôAllo ‚ÄôAllo!(series4)",‚ÄôKimHartman‚Äô,"‚ÄôAllo‚ÄôAllo! (series6)",‚ÄôJeremyLloyd‚Äô] Thought3:Tofindtheactorwhowasbestknown forwhichroleon"‚ÄôAllo‚ÄôAllo!", Icansearch GordenKaye. Action3:Search[GordenKaye] Observation3: GordonIrvingKaye(7April 1941‚Äì23January2017), knownprofessionally asGordenKaye, was anEnglish actor, bestknownforplayingwomanisingcaf√©owner Ren√©Artoisinthetelevisioncomedyseries‚ÄôAllo ‚ÄôAllo!. Thought4:GordenKayewasbestknownfor playingwomanisingcaf√©ownerRen√©Artoisin thetelevisioncomedyseries"‚ÄôAllo‚ÄôAllo!".So theanswerisReneArtois. Action4:Finish[ReneArtois] Observation4:AnswerisINCORRECT Trial#2:Question:Grown-Upsstarredtheactor whowasbestknownforwhichroleon"‚ÄôAllo ‚ÄôAllo!"? Thought1: Ineedtosearchtheactorwhowas bestknownforaroleon"‚ÄôAllo‚ÄôAllo!"andfind outwhatroletheywerebestknownfor. Action1:Search[Grown-Ups] Observation1: Grown-Ups isa1980British BBCtelevisionfilmdevisedanddirectedby MikeLeigh. It starsLesleyManville, Philip Davis,BrendaBlethyn,JanineDuvitski,Lindsay DuncanandSamKelly. ItwaseditedbyRobin SalesandproducedbyLouisMarksfortheBBC, andoriginallyshownonBBC2on28November 1980. Thought2:Theparagraphdoesnotmentionthe actorwhowasbestknownforaroleon"‚ÄôAllo ‚ÄôAllo!".Ineedtosearchtheactor‚Äôsnameinstead. Action2:Search[SamKelly] Observation2:RogerMichaelKelly(19December1943‚Äì14June2014),knownbythestage nameSamKelly,wasanEnglishactorwhoappearedinfilm,television,radioandtheatre.He isbestknownforhisrolesasCaptainHansGeeringin‚ÄôAllo‚ÄôAllo!,WarreninPorridge,Samin OntheUp,andTedLiversidgeinBarbara. Thought3:SamKellyisbestknownforhisrole asCaptainHansGeeringin"‚ÄôAllo‚ÄôAllo!", so theanswerisCaptainHansGeering. Action3:Finish[CaptainHansGeering] Observation3:AnswerisCORRECT Reflection:Isearchedthewrongtitlefortheshow,"‚ÄôAllo‚ÄôAllo!",whichresultedinnoresults.I shouldhavesearchedtheshow‚Äôsmaincharacter,GordenKaye,tofindtherolehewasbestknownfor intheshow. Figure7:TwoHotPotQAtrialswithinthesameenvironmentandtask.TheReflexion+ReActagent usesself-reflectiontodetermineabettersearchmethodforthenexttrial. 17D.2 Chain-of-Thought+Reflexion Trial#1:Question:WhatprofessiondoesJohn LanchesterandAlanDeanFosterhaveincommon? Thought1:Let‚Äôsthinkstepbystep.JohnLanchester isanovelist, journalist, andcritic. Alan DeanFosterisanovelistandscreenwriter. So theprofessionJohnLanchesterandAlanDean Fosterhaveincommonisnovelistandscreenwriter. Action1:Finish[novelistandscreenwriter] Observation1:AnswerisINCORRECT Trial#2:Question:WhatprofessiondoesJohn LanchesterandAlanDeanFosterhaveincommon? Thought1:Let‚Äôsthinkstepbystep.JohnLanchesterisanovelist,journalist,andfinancialjournalist.AlanDeanFosterisanovelistandscreenwriter. SotheprofessionJohnLanchesterand AlanDeanFosterhaveincommonisnovelist. Action1:Finish[novelist] Observation1:AnswerisCORRECT Reflection:MyreasoningfortheprofessionJohnLanchesterandAlanDeanFosterhaveincommon failedbecauseIincorrectlyassumedthat theybothhadthesameprofession. Inthefuture,when attemptingthisquestion,Ishouldfocusonbetterresearchingtheindividualbackgroundsofboth authorstoensurethatIamaccuratelyidentifyingtheirprofessions.Additionally,Ishouldconsider thepossibilitythattheymayhavemultipleprofessionsincommon. D.3 HotPotQAChain-of-Thought(GT)+Reflexion Context:TheBattleofWhitePlainswasabattleintheNewYorkandNewJerseycampaignofthe AmericanRevolutionaryWarfoughtonOctober28,1776,nearWhitePlains,NewYork.Following theretreat... Trial#1:Question:Whatwasaseriesofbattles duringtheRevolutionaryWar,forcontrolofNew YorkCityandthestateofNewJersey,foughton October28,1776nearWhitePlains,NewYork? Thought1:ThequestionisaskingaboutaseriesofbattlesduringtheRevolutionaryWarfor controlofNewYorkCityandthestateofNew Jersey,foughtonOctober28,1776nearWhite Plains,NewYork.Fromthecontext,Iknowthat thiswastheBattleofWhitePlains. Action1:Finish[BattleofWhitePlains] Observation1:AnswerisINCORRECT Trial#2:Question:Whatwasaseriesofbattles duringtheRevolutionaryWar,forcontrolofNew YorkCityandthestateofNewJersey,foughton October28,1776nearWhitePlains,NewYork? Thought1:Thequestionisaskingforaseriesof battlesduringtheRevolutionaryWar,forcontrol ofNewYorkCityandthestateofNewJersey, foughtonOctober28,1776nearWhitePlains, NewYork. Basedonthecontext, Iknowthat thisseriesofbattleswastheNewYorkandNew Jerseycampaign. Action1:Finish[TheNewYorkandNewJersey campaign] Observation1:AnswerisCORRECT Reflection:UponreflectingontheincorrectanswerIprovided,IrealizethatImaynothaveprovided enoughcontexttoaccuratelyanswerthequestion.Thequestionaskedforaseriesofbattles,butI onlyprovidedthenameofonebattle.Tobetteranswerthisquestioninthefuture,Iwillmakesureto providemorecontext,suchasthenameofthecampaign,whichwastheNewYorkandNewJersey campaign,andthefactthatitwasaseriesofbattles.Additionally,Iwillmakesuretoincludethe datesofthebattles,aswellasthelocations,toensurethatmyansweriscomprehensiveandaccurate. 18D.4 HotPotQAepisodicmemory(EPM)ablationprompts D.4.1 (EPM)Chain-of-Thought+Reflexion Trial#1:Question:WhichofJonnyCraigand PeteDohertyhasbeenamemberofmorebands ? Thought 1: Let‚Äôs thinkstepbystep. Jonny Craighasbeenamemberofsixbands,including Emarosa,DanceGavinDance,andSlaves.Pete Dohertyhasbeenamemberofsevenbands,includingTheLibertines,Babyshambles,andThe PutaMadres.SoPeteDohertyhasbeenamemberofmorebands. Action1:Finish[PeteDoherty] Observation1:AnswerisINCORRECT Trial#2:Question:WhichofJonnyCraigand PeteDohertyhasbeenamemberofmorebands ? Thought1: Let‚Äôsthinkstepbystep. Iwill researchthepastandcurrentbandsofbothmusicianstoensurethatIamaccuratelycomparing theirbandmemberships. After researching, I havefoundthatJonnyCraighasbeenamemberofsevenbands, includingEmarosa,Dance GavinDance,andSlaves.PeteDohertyhasbeen amemberofsevenbands, includingTheLibertines,Babyshambles,andThePutaMadres.So JonnyCraighasbeenamemberofmorebands. Action2:Finish[JonnyCraig] Observation2:AnswerisCORRECT Reflection:MyreasoningforwhichmusicianhasbeenamemberofmorebandsfailedbecauseIdid nottakeintoaccountthatJonnyCraighasbeenamemberofmorebandsinthepast,eventhoughPete Dohertyiscurrentlyamemberofmorebands.Inthefuture,whenattemptingthisquestion,Ishould focusonresearchingthepastandcurrentbandsofbothmusicianstoensurethatIamaccurately comparingtheirbandmemberships. D.4.2 (EPM)Chain-of-Thought(GT)+Reflexion Context:HariBahadurBasnet isaNepalesepolitician.HeistheheadoftheForeignRelations DepartmentoftheRastriyaJanashaktiParty.BasnetholdsaM.Sc.inEngineering. AMasterofScience(Latin:"MagisterScientiae";abbreviatedMS,M.S.,MSc,M.Sc.,MSci,M.Sci., ScM,Sc.M.,SciMorSci.M.)isamaster‚Äôsdegreeinthefieldofscience... Trial#1:Question:Theheadof theForeign RelationsDepartmentoftheRastriyaJanashakti PartyholdsadegreethatcanbeabbreviatedMS, M.S.,orScM,inwhatfield? (Previoustrajectory) Thought1: Thequestionisaskingwhatfield of study the head of the ForeignRelations Department of theRastriya Janashakti Party holdsadegree in. Fromthecontext, Iknow that thedegreeisaMasterofSciencedegree, whichistypicallygrantedforstudiesinsciences, engineering,andmedicine. Action 1: Finish[Sciences, Engineering, and Medicine] Observation1:AnswerisINCORRECT Trial#2:Question:TheheadoftheForeignRelationsDepartmentof theRastriyaJanashakti PartyholdsadegreethatcanbeabbreviatedMS, M.S.,orScM,inwhatfield? (Previoustrajectory) Thought1: Thequestionisaskingwhatfield ofstudytheheadoftheForeignRelationsDepartmentoftheRastriyaJanashaktiPartyholds adegreein. Fromthecontext, Iknowthat the degreeisaMasterofSciencedegree,whichis typicallygrantedforstudiesinsciences, engineering,andmedicine.However, thequestion isaskingfor thefieldofstudy,not thedegree. Basedonthecontext,IknowthatHariBahadur BasnetholdsaM.Sc. inEngineering. Action1:Finish[Engineering] Observation1:AnswerisCORRECT Reflection:Forthefirsttrial,Imisunderstoodthequestion.Ithoughtthequestionwasaskingfora categoryofdegrees,ratherthanaspecificdegree.Ilearnedfrommymistakeandinthesecondtrial,I focusedmorecloselyonthequestiontounderstandthatitwasaskingforaspecificfieldofstudy.